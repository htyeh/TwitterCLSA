>Embeddings
En-De: weighted averaging from https://github.com/pedrada88/crossembeddings-twitter

>Links to datasets
SemEval: https://www.dropbox.com/s/byzr8yoda6bua1b/2017_English_final.zip?
CLARIN: https://www.clarin.si/repository/xmlui/handle/11356/1054
SB-10K: link: https://www.spinningbytes.com/resources/germansentiment/

>Available datasets (neg/neu/pos)
<EN> SemEval13-16 full (train 5383/15533/13724; dev 1153/3328/2941; test 1153/3328/2941)
<EN> SemEval13-16 oversampled (train 14079/15533/13724; dev 1153/3328/2941; test 1153/3328/2941) * merged with CLARIN neg train
<EN> SemEval13-16 downsampled (train 5383/6000/6000; dev 1153/3328/2941; test 1153/3328/2941)
<EN> CLARIN.si full (train 8696/16590/9415; dev 1863/3555/2019; test 1863/3555/2019)
<EN> CLARIN.si oversampled (train 16590/16590/16590; dev 1153/3328/2941; test 1153/3328/2941)
<EN> CLARIN.si downsampled (train 8696/9000/9415; dev 1863/3555/2019; test 1863/3555/2019)
<DE> CLARIN.si full (train 9697/29522/13643; dev 2078/6326/2923; test 2078/6326/2923)
<DE> SB-10k (989 neg/4131 neu/1509 pos)
<DE> SB-10k balanced (989 neg/1500 neu/1509 pos)
>Train/Dev/Test split: 70/15/15

>Tweet cleansing
1) removed 'Not Available' ones
2) contractions are kept (own embeddings)
3) hashtags kept (might contain sentiment info)
4) to lowercase
5) removed @mentions/links
6) emoticon conversion (Keras Tokenizer does not encode emoticons; all mapped to English expressions acc. following link)
https://towardsdatascience.com/twitter-sentiment-analysis-using-fasttext-9ccd04465597
7) punctuation removal
8) converted \u2019 (to '), \u002c (to ,)

-------------------------------------------------------------------------------------------------------------------------------------------
COMPLETE EXPERIMENT DESCRIPTION
-------------------------------------------------------------------------------------------------------------------------------------------
1) baseline: train/dev/test on De
  a) scale De data & test for overfitting
2) lower bound settings (suggested in https://arxiv.org/pdf/1905.07358.pdf)
  a) initialize EmbLayer, test on De directly
  b) fixed [En BWE], test on De directly (sanity check for overlapping words)
  c) always predict majority class
3) fixed [EnDe BWE] (classical fine-tuning)
4) trainable [EnDe BWE] (classical fine-tuning)
  a) full De fine-tuning
  b) downsized De fine-tuning
5) fixed [EnDe BWE] (zero-shot)
6) *trainable [EnDe BWE] (zero-shot)
  a) reasons for underperformance
7) combine fixed + trainable [EnDe BWE]
  a) *zero-shot
  b) classical
8) BWE + architecture joint enhancement
  a) EmbEnhancer (train embs, then rest of architecture)
  b) ArchEnhancer (train architecture, then implement BWE tuning)
9) (optional) sentiment-tuning potential: are tuned embs really worse?
  a) building vocab only on train-data is not useful since most embs will be updated less than others
10) trainable [En BWE], map [De BWE] to [En BWE]
  a) MUSE/vecmap
  b) compare mapping evaluation
11) (optional) train En EmbLayer, map De to En space

* only En part of [EnDe BWE] get updates (no training on De vocab)
-------------------------------------------------------------------------------------------------------------------------------------------
EXPERIMENT SETTINGS IN THIS LOG
-------------------------------------------------------------------------------------------------------------------------------------------
** results recorded in this log:
  * compare basic scores on different datasets
  * compare basic scores on different model architectures

(C-classical transfer, Z-zero-shot transfer)
Z1: En train/dev/test, EmbLayer; De test (zero-shot) -> De vocab should have random embeddings
ZM: En train/dev/test, [En BWE]; De test (zero-shot) -> De vocab should have all 0 embeddings
Z2: En train/dev/test, [EnDe BWE] fixed; De test (zero-shot)
Z3: En train/dev/test, [EnDe BWE] trainable; De test (zero-shot)


-------------------------------------------------------------------------------------------------------------------------------------------
<model architecture: bilstm(128) + dropout(0.2) + dense(64) + dense(64)> <batch=64> <setting=Z2>
train             dev             test(En)      test(De)      F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)  pred. comment
SemEval(full)     SemEval(full)   SemEval(full) CLARIN(full)  64.16         51.26         60.48         50.23
SemEval(oversamp) SemEval(full)   SemEval(full) CLARIN(full)  62.76         29.67         57.18         29.01         De neg-skewed
SemEval(oversamp) SemEval(full)   SemEval(full) SB-10k(full)  64.17         25.33         60.45         25.22         De neg-skewed
SemEval(downsamp) SemEval(full)   SemEval(full) CLARIN(full)  58.36         46.43         57.21         46.77
CLARIN(full)      CLARIN(full)    CLARIN(full)  CLARIN(full)  63.13         56.71         61.31         50.85
CLARIN(oversamp)  CLARIN(full)    CLARIN(full)  CLARIN(full)  60.74         51.38         60.62         49.87
CLARIN(downsamp)  CLARIN(full)    CLARIN(full)  CLARIN(full)  59.83         51.38         59.39         50.20
-------------------------------------------------------------------------------------------------------------------------------------------
<model architecture: Medium post (https://medium.com/@panghalarsh/sentiment-analysis-in-python-using-keras-glove-twitter-word-embeddings-and-deep-rnn-on-a-combined-580646cb900a)> <batch=64> <setting=Z2>
train             dev             test(En)      test(De)      F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)  pred. comment   epoch
SemEval(full)     SemEval(full)   SemEval(full) CLARIN(full)  63.69         58.37         57.34         48.70                         12
SemEval(oversamp) SemEval(full)   SemEval(full) CLARIN(full)  63.58         24.99         57.62         21.56         De neg-skewed   9
SemEval(oversamp) SemEval(full)   SemEval(full) SB-10k(full)  59.46         28.89         53.40         26.99         De neg/En neu skewed  3
SemEval(downsamp) SemEval(full)   SemEval(full) CLARIN(full)  61.45         53.67         57.42         47.25                         9
CLARIN(full)      CLARIN(full)    CLARIN(full)  CLARIN(full)  62.75         58.39         60.66         48.30         De neu-skewed   9
CLARIN(oversamp)  CLARIN(full)    CLARIN(full)  CLARIN(full)  61.62         59.01         59.30         48.02         De neu-skewed   6
CLARIN(downsamp)  CLARIN(full)    CLARIN(full)  CLARIN(full)  61.08         57.77         59.81         50.14                         5
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> (uncleansed) SemEval full train (5383 neg/15533 neu/13724 pos)
<EN> (uncleansed) SemEval full dev (1153 neg/3328 neu/2941 pos)
<EN> (uncleansed) SemEval full test (1153 neg/3328 neu/2941 pos)
<DE> (uncleansed) CLARIN full test (2078 neg/6326 neu/2923 pos)
setting MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
Z1      30      128     0.2     2*64      -         64    1     62.19         55.15         54.32         28.28
ZM      30      128     0.2     2*64      -         64    7     63.35         52.53         59.75         40.85
Z2      30      128     0.2     2*64      -         64    8     63.60         51.67         59.69         49.73
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> SemEval full train (5383 neg/15533 neu/13724 pos)
<EN> SemEval full dev (1153 neg/3328 neu/2941 pos)
<EN> SemEval full test (1153 neg/3328 neu/2941 pos)
<DE> CLARIN full test (2078 neg/6326 neu/2923 pos)
Z1      30      128     0.2     2*64      -         64    1     62.14         52.14         57.69         34.79
ZM      30      128     0.2     2*64      -         64    8     63.66         51.02         59.17         40.55
Z2      30      128     0.2     2*64      -         64    7     64.16         51.26         60.48         50.23
Z3      30      128     0.2     2*64      -         64    1     63.84         51.98         59.25         47.65
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> SemEval oversampled train (oversampled w/ CLARIN neg train) (14079 neg/15533 neu/13724 pos)
<EN> SemEval full dev (1153 neg/3328 neu/2941 pos)
<EN> SemEval full test (1153 neg/3328 neu/2941 pos)
<DE> CLARIN full test (2078 neg/6326 neu/2923 pos)
setting MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)  pred. comment
Z1      30      128     0.2     2*64      -         64    1     60.30         23.96         57.20         18.84         De neg-skewed
ZM      30      128     0.2     2*64      -         64    6     62.49         24.95         59.60         22.10         De neg-skewed
Z2      30      128     0.2     2*64      -         64    5     62.76         29.67         57.18         29.01         De neg-skewed
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> SemEval oversampled train (oversampled w/ CLARIN neg train) (14079 neg/15533 neu/13724 pos)
<EN> SemEval full dev (1153 neg/3328 neu/2941 pos)
<EN> SemEval full test (1153 neg/3328 neu/2941 pos)
<DE> SB-10k full test (989 neg/4131 neu/1509 pos)
setting MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)  pred. comment
Z1      30      128     0.2     2*64      -         64    1     59.11         23.29         57.00         19.05         De neg-skewed
ZM      30      128     0.2     2*64      -         64    8     64.23         23.00         60.41         21.29         De neg-skewed
Z2      30      128     0.2     2*64      -         64    6     64.17         25.33         60.45         25.22         De neg-skewed
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> SemEval downsampled train (5383 neg/6000 neu/6000 pos)
<EN> SemEval full dev (1153 neg/3328 neu/2941 pos)
<EN> SemEval full test (1153 neg/3328 neu/2941 pos)
<DE> CLARIN full test (2078 neg/6326 neu/2923 pos)
setting MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
Z1      30      128     0.2     2*64      -         64    1     49.20         42.18         48.42         31.41
ZM      30      128     0.2     2*64      -         64    4     57.67         46.51         56.09         41.06
Z2      30      128     0.2     2*64      -         64    9     58.36         46.43         57.21         46.77
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> CLARIN downsampled train (8696 neg/9000 neu/9415 pos)
<EN> CLARIN full dev (1863 neg/3555 neu/2019 pos)
<EN> CLARIN full test (1863 neg/3555 neu/2019 pos)
<DE> CLARIN full test (2078 neg/6326 neu/2923 pos)
setting MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
Z1      30      128     0.2     2*64      -         64    1     55.91         49.46         56.07         35.17
ZM      30      128     0.2     2*64      -         64    8     59.91         48.88         59.69         43.26
Z2      30      128     0.2     2*64      -         64    4     59.83         51.38         59.39         50.20
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> CLARIN full train (8696 neg/16590 neu/9415 pos)
<EN> CLARIN full dev (1863 neg/3555 neu/2019 pos)
<EN> CLARIN full test (1863 neg/3555 neu/2019 pos)
<DE> CLARIN full test (2078 neg/6326 neu/2923 pos)
setting MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
Z1      30      128     0.2     2*64      -         64    1     60.99         54.78         59.21         32.43
ZM      30      128     0.2     2*64      -         64    10    63.19         54.08         61.53         41.57
Z2      30      128     0.2     2*64      -         64    8     63.13         56.71         61.31         50.85
Z3      30      128     0.2     2*64      -         64    1     62.03         55.38         60.71         49.62
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> CLARIN full train (8696 neg/16590 neu/9415 pos)
<EN> CLARIN full dev (1863 neg/3555 neu/2019 pos)
<EN> CLARIN full test (1863 neg/3555 neu/2019 pos)
<DE> SB-10k full test (989 neg/4131 neu/1509 pos)
setting MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)  pred. comment
Z1      30      128     0.2     2*64      -         64    1     60.65         65.44         59.39         44.94         De neu-skewed
ZM      30      128     0.2     2*64      -         64    10    63.36         66.55         61.82         53.23
Z2      30      128     0.2     2*64      -         64    10    63.75         66.58         62.25         59.29
Z3      30      128     0.2     2*64      -         64    1     62.97         68.38         61.27         58.76
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> CLARIN oversampled train (16590 neg/16590 neu/16590 pos)
<EN> CLARIN full dev (1863 neg/3555 neu/2019 pos)
<EN> CLARIN full test (1863 neg/3555 neu/2019 pos)
<DE> CLARIN full test (2078 neg/6326 neu/2923 pos)
setting MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)  pred. comment
Z1      30      128     0.2     2*64      -         64    1     58.85         51.98         58.54         35.11         De neu-skewed
ZM      30      128     0.2     2*64      -         64    9     61.09         49.48         60.95         43.06
Z2      30      128     0.2     2*64      -         64    6     60.74         51.38         60.62         49.87
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> CLARIN downsampled train (8696 neg/9000 neu/9415 pos)
<EN> CLARIN full dev (1863 neg/3555 neu/2019 pos)
<EN> CLARIN full test (1863 neg/3555 neu/2019 pos)
<DE> SB-10k full test (989 neg/4131 neu/1509 pos)
setting MAXLEN  bilstm  dropout dense     dropouts  batch  epoch  F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
Z1      30      32      -       2*64      -         32      1     57.46         62.80         57.94         46.96
Z1      30      64      0.5     2*64      -         32      1     55.43         60.90         55.63         48.05
Z1      30      64      -       2*64      -         32      1     54.50         60.28         54.71         48.84
Z1      30      64      -       2*128     -         32      1     54.44         58.25         54.49         46.69
Z1      30      128     -       2*128     -         32      1     56.63         62.83         56.79         46.99
ZM      30      64      -       2*64      -         32      8     58.69         57.78         58.84         50.84
ZM      30      64      -       2*128     -         32      6     57.86         57.38         58.00         51.08
ZM      30      128     -       2*128     -         32      5     58.09         60.02         58.21         52.83
Z2      10      -       -       1*128     -         16      2     57.02         59.10
Z2      10      -       -       3*128     -         16      1     57.04         60.36
Z2      30      32      -       2*32      0.4,-     32      6     58.41         59.12
Z2      30      32      -       2*64      0.4,-     32      9     58.53         58.72         57.86         54.01
Z2      30      64      -       2*64      0.4,-     32      8     60.00         60.72         59.56         55.78
Z2      30      64      -       2*128     0.4,-     32      7     60.05         60.82
Z2      30      128     0.8     2*64      0.4,-     32      8     59.71         59.08
Z2      30      128     0.5     2*64      0.4,-     32      6     59.54         59.57
Z2      30      128     0.1     2*64      0.4,-     32      7     60.64         62.50
Z2      30      128     0.1     2*64      0.2,-     32      6     61.18         62.90
Z2      30      128     0.1     128+64    0.4,0.2   32      10    58.83         59.38
Z2      30      128     0.1     128+64    0.4,-     32      7     61.18         64.26
Z2      30      128     0.1     128+64    0.2,-     32      5     59.69         60.28
Z2      30      128     0.1     2*128     0.4,-     32      10    60.35         63.10
Z2      30      128     0.1     2*128     0.2,-     32      5     60.18         60.89
Z2      30      128     0.1     2*128     -,-       32      2     60.21         59.54
Z2      30      128     -       2*128     0.4,-     32      5     60.37         61.76
Z2      30      128     0.1     2*256     0.4,-     32      2     58.09         59.38
Z2      30      256     -       256+128   0.4,-     32      5     59.98         61.63
Z2      30      256     -       2*256     0.4,-     32      7     60.75         62.62
Z2      30      256     0.1     2*256     0.2,-     32      7     59.86         60.82
Z2      10      512     0.5     2*128     -         16      7     55.63         55.90
Z2      10      128     0.5     4*32      -         32      9     56.77         58.55
Z2      10      256     0.5     4*64      -         32      6     56.75         58.80


>Issues/experiments to address
Tokenizer does not recognize De vocabs
  - fit_on_texts on both train(En) and test(De)
En: only vocab seen in training updated (generalization problem)
De embs not updated in zero-shot
1 reason for bad performance with updated embs: unable to generalize when only training embs updated
  - fit Tokenizer on train vocab only: train these into sentiment vectors
merged BWE settings: architecture only trained for 1 epoch
  - try using the updated BWE with an architecture trained on both BWE fixed
generally involving trainable embeddings: architecture stops being trained after 1 epoch
  - train with BWE fixed -> train with trainable for 1 epoch


>>Older results
>Experiments using CLARIN En full (12422 neg/23700 neu/13453 pos)
* best epoch num reported by Callback
setting MAXLEN  bilstm  lstm  dropout   dense   dropouts  batch   epoch F1-micro(En)  F1-micro(De)
M1      30      -       -     -         2*128   -         32      1     60.23         -
M1      30      256     -     0.9       2*128   -         32      1     60.62         -
M1      30      256     -     0.5       2*128   -         32      1     61.70         -
M1      30      256     -     0.2       2*128   -         32      1     61.48         -
M1      30      128     -     0.9       2*128   -         32      2     60.84         -
M1      30      128     -     0.5       2*128   -         32      1     60.67         -
M1      30      64      -     0.5       2*128   -         32      1     61.37         -
M1      30      64      -     0.5       2*256   -         32      1     59.90         -
M1      30      64      -     0.5       2*64    -         32      1     60.06         -
M1      30      32      -     0.5       2*32    -         32      1     61.22         -

M2      30      128     -     0.9       2*128   -         32      13    60.21         -
M2      30      128     -     0.5       2*128   -         32      14    60.39         -
M2      30      -       128   0.5       2*128   -         32      11    60.33         -
M2      30      -       64    0.5       2*64    -         32      13    59.15         -
M2      30      64      -     0.9       2*32    -         32      12    59.98         -
M2      30      64      -     0.9       2*32    -         128     15    58.32         -
M2      30      128     -     0.5       2*32    -         16      9     60.48         -
M2      30      128     -     0.5       2*64    -         16      11    59.94         -
M2      30      128     -     0.5       2*128   -         8       8     61.03         -
M2      30      128     -     0.2       2*128   -         8       4     60.56         -
M2      30      256     -     0.5       2*128   -         16      11    60.33         -
