>Embeddings
En-De: weighted averaging from https://github.com/pedrada88/crossembeddings-twitter

>Available datasets
EN: SemEval13-16 (7689 neg/22189 neu/19606 pos)
EN: SemEval13-16 balanced (10000 neg/10000 neu/ 10000 pos)
EN: CLARIN.si (12422 neg/23700 neu/13453 pos)
EN: CLARIN.si balanced (12422 neg/~13000 neu/13453 pos)
DE: SB-10k (989 neg/4131 neu/1509 pos)
DE: SB-10k balanced used for zero-shot testing (989 neg/~1500 neu/1509 pos)
DE: CLARIN.si (13853 neg/42174 neu/19489 pos)

>Data split
for CLARIN: 70% train, take last 15%/15% as dev/test
for SemEval+CLARIN 70/15/15
for CLARIN balanced: removed the last a few thousand Tweets from neutral train/dev/test Tweets

>Tweet cleansing
1) removed Not Available ones
2) contractions are kept (own embeddings)
3) hashtags kept (might express sentiment)
4) all lowercase
5) removed @mentions/links
6) emoticon conversion (all mapped to English expressions as these words have embs in the joint BWE)
https://towardsdatascience.com/twitter-sentiment-analysis-using-fasttext-9ccd04465597
7) removed punctuation (after emoticon conversion they can be deleted also from between two words e.g. '...')
8) converted \u2019 (to '), \u002c (to ,)

>Experiments
1) baseline: train/test on De data
2) fixed [En BWE], testing on De directly (sanity check for overlapping words)
2) fixed [EnDe BWE] (classical fine-tuning)
2) *fixed [EnDe BWE] (zero-shot)
3) *trainable [EnDe BWE] (classical fine-tuning)
3) trainable [EnDe BWE] (zero-shot)
4) combine fixed & trainable [EnDe BWE]
5) update [En BWE], map to [De BWE]
6) search for better architecture (e.g. adding a Conv layer) to achieve better results
* only En part of [EnDe BWE] get updates (no training on De vocab)

>Experiment settings (M-monolingual, C-classical transfer, Z-zero-shot transfer)
M1: En train/dev/test, EmbLayer
M2: En train/dev/test, [EnDe BWE] fixed
M3: En train (27000, val 0.1) / En test (3000), [EnDe BWE] trainable
Z1: En train/dev/test, EmbLayer; De test (zero-shot)
Z2: En train/dev/test, [EnDe BWE] fixed; De test (zero-shot)

>Experiments using CLARIN En imbalanced (12422 neg/23700 neu/13453 pos)
* best epoch num reported by Callback
* - means did not implement
setting MAXLEN  bilstm  lstm  dropout   dense   dropouts  batch   epoch F1(En)    F1(De)
M1      30      -       -     -         2*128   -         32      1     60.23     -
M1      30      256     -     0.9       2*128   -         32      1     60.62     -
M1      30      256     -     0.5       2*128   -         32      1     61.70     -
M1      30      256     -     0.2       2*128   -         32      1     61.48     -
M1      30      128     -     0.9       2*128   -         32      2     60.84     -
M1      30      128     -     0.5       2*128   -         32      1     60.67     -
M1      30      64      -     0.5       2*128   -         32      1     61.37     -
M1      30      64      -     0.5       2*256   -         32      1     59.90     -
M1      30      64      -     0.5       2*64    -         32      1     60.06     -
M1      30      32      -     0.5       2*32    -         32      1     61.22     -

M2      30      128     -     0.9       2*128   -         32      13    60.21     -
M2      30      128     -     0.5       2*128   -         32      14    60.39     -
M2      30      -       128   0.5       2*128   -         32      11    60.33     -
M2      30      -       64    0.5       2*64    -         32      13    59.15     -
M2      30      64      -     0.9       2*32    -         32      12    59.98     -
M2      30      64      -     0.9       2*32    -         128     15    58.32     -
M2      30      128     -     0.5       2*32    -         16      9     60.48     -
M2      30      128     -     0.5       2*64    -         16      11    59.94     -
M2      30      128     -     0.5       2*128   -         8       8     61.03     -
M2      30      128     -     0.2       2*128   -         8       4     60.56     -
M2      30      256     -     0.5       2*128   -         16      11    60.33     -

>Experiments using CLARIN En balanced (12422 neg/13000 neu/13453 pos)
* best epoch num reported by Callback
* - means did not implement
setting MAXLEN  bilstm  dropout dense     batch  epoch  F1(En)    F1(De)
M1      30      64      0.5     2*128     32      1     57.14     -
M1      30      128     0.5     2*128     32      1     60.71     -
M1      30      256     0.9     2*128     32      1     56.61     -
M1      30      256     0.5     2*128     32      1     59.03     -
M1      30      256     0.2     2*128     32      1     60.78     -
M1      30      128     0.5     2*256     32      1     59.81     -
Z1      10      64      0.5     2*32      8       1     57.75     45.52
Z1      10      128     0.5     2*64      8       1     56.99     48.02
Z1      10      128     0.5     2*128     8       1     58.09     ?
M2      30      128     0.2     2*128     8       4     57.70     -
Z2      10      -       -       1*128     16      2     57.02     59.10
Z2      10      -       -       3*128     16      1     57.04     60.36
Z2      10      128     0.5     4*32      32      9     56.77     58.55
Z2      10      256     0.5     4*64      32      6     56.75     58.80
Z2      10      512     0.5     2*128     16      7     55.63     55.90


>Experiments using SemEval En balanced
* best epoch # reported by Callback
setting MAXLEN  bilstm  dropout dense     batch  epoch  F1(En)    F1(De)



>Issues to address
1) Tokenizer does not recognize De vocabs
  - fit_on_texts on both train(En) and test(De)
2) training only with En -> De embs not updated
