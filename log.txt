>Embeddings
En-De: weighted averaging from https://github.com/pedrada88/crossembeddings-twitter

>Available datasets
EN: SemEval13-16 (7689 neg/22189 neu/19606 pos)
EN: SemEval13-16 balanced (10000 neg/10000 neu/ 10000 pos)
EN: CLARIN.si (12422 neg/23700 neu/13453 pos)
EN: CLARIN.si balanced (12422 neg/~13000 neu/13453 pos)
DE: SB-10k (989 neg/4131 neu/1509 pos)
DE: SB-10k balanced used for zero-shot testing (989 neg/~1500 neu/1509 pos)
DE: CLARIN.si (13853 neg/42174 neu/19489 pos)

>Datasets in use
Train: balanced CLARIN En 8696 neg/9000 neu/9415 pos
Dev: imbalanced CLARIN En 1863 neg/3555 neu/2019 pos
Test: imbalanced CLARIN En 1863 neg/3555 neu/2019 pos
Test: imbalanced De 989 neg/4131 neu/1509 pos

>Data split
70/15/15 (last 30% Tweets taken directly for dev & test)

>Tweet cleansing
1) removed Not Available ones
2) contractions are kept (own embeddings)
3) hashtags kept (might express sentiment)
4) all lowercase
5) removed @mentions/links
6) emoticon conversion (all mapped to English expressions as these words have embs in the joint BWE)
* Keras Tokenizer does not encode emoticons
https://towardsdatascience.com/twitter-sentiment-analysis-using-fasttext-9ccd04465597
7) removed punctuation (after emoticon conversion they can be deleted also from between two words e.g. '...')
8) converted \u2019 (to '), \u002c (to ,)

>Experiments
1) baseline: train/test on De data
2) fixed [En BWE], testing on De directly (sanity check for overlapping words)
2) fixed [EnDe BWE] (classical fine-tuning)
2) *fixed [EnDe BWE] (zero-shot)
3) *trainable [EnDe BWE] (classical fine-tuning)
3) trainable [EnDe BWE] (zero-shot)
4) combine fixed & trainable [EnDe BWE]
5) update [En BWE], map to [De BWE]
6) search for better architecture (e.g. adding a Conv layer) to achieve better results
* only En part of [EnDe BWE] get updates (no training on De vocab)

>Experiment settings (M-monolingual, C-classical transfer, Z-zero-shot transfer)
M1: En train/dev/test, EmbLayer
M2: En train/dev/test, [EnDe BWE] fixed
M3: En train/dev/test [EnDe BWE] trainable
Z1: En train/dev/test, EmbLayer; De test (zero-shot) -> De vocab should have random embeddings
ZM: En train/dev/test, [En BWE]; De test (zero-shot) -> De vocab should have all 0 embeddings
Z2: En train/dev/test, [EnDe BWE] fixed; De test (zero-shot)
Z3: M3: En train/dev/test [EnDe BWE] trainable; De test (zero-shot)

>Experiments using CLARIN En balanced (12422 neg/13000 neu/13453 pos) and De imbalanced (989 neg/4131 neu/1509 pos)
* best epoch num reported by Callback
* - means did not implement
following tests evaluated with balanced En/De test sets
setting MAXLEN  bilstm  dropout dense     dropouts  batch  epoch  F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
Z1      10      64      0.5     2*32      -         8       1     57.75         45.52         -             -
Z1      10      128     0.5     2*64      -         8       1     56.99         48.02         -             -
Z1      10      128     0.5     2*128     -         16      1     57.72         49.17         -             -
Z1      30      128     0.5     2*128     -         16      1     59.78         50.45         -             -
Z1      30      256     0.5     256+128   -         16      1     58.13         49.52         -             -

the rest evaluated with full (imbalanced) En/De test sets
setting MAXLEN  bilstm  dropout dense     dropouts  batch  epoch  F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
Z1      30      32      -       2*64      -         32      1     57.46         62.80         57.94         46.96
Z1      30      64      0.5     2*64      -         32      1     55.43         60.90         55.63         48.05
Z1      30      64      -       2*64      -         32      1     54.50         60.28         54.71         48.84
Z1      30      64      -       2*128     -         32      1     54.44         58.25         54.49         46.69
Z1      30      128     -       2*128     -         32      1     56.63         62.83         56.79         46.99

ZM      30      64      -       2*64      -         32      8     58.69         57.78         58.84         50.84
ZM      30      64      -       2*128     -         32      6     57.86         57.38         58.00         51.08
ZM      30      128     -       2*128     -         32      5     58.09         60.02         58.21         52.83

setting MAXLEN  bilstm  dropout dense     dropouts  batch  epoch  F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
Z2      10      -       -       1*128     -         16      2     57.02         59.10
Z2      10      -       -       3*128     -         16      1     57.04         60.36
Z2      30      32      -       2*32      0.4,-     32      6     58.41         59.12
Z2      30      32      -       2*64      0.4,-     32      9     58.53         58.72         57.86         54.01
Z2      30      64      -       2*64      0.4,-     32      8     60.00         60.72         59.56         55.78
Z2      30      64      -       2*128     0.4,-     32      7     60.05         60.82
Z2      30      128     0.8     2*64      0.4,-     32      8     59.71         59.08
Z2      30      128     0.5     2*64      0.4,-     32      6     59.54         59.57
Z2      30      128     0.1     2*64      0.4,-     32      7     60.64         62.50
Z2      30      128     0.1     2*64      0.2,-     32      6     61.18         62.90
Z2      30      128     0.1     128+64    0.4,0.2   32      10    58.83         59.38
Z2      30      128     0.1     128+64    0.4,-     32      7     61.18         64.26
Z2      30      128     0.1     128+64    0.2,-     32      5     59.69         60.28
Z2      30      128     0.1     2*128     0.4,-     32      10    60.35         63.10
Z2      30      128     0.1     2*128     0.2,-     32      5     60.18         60.89
Z2      30      128     0.1     2*128     -,-       32      2     60.21         59.54
Z2      30      128     -       2*128     0.4,-     32      5     60.37         61.76
Z2      30      128     0.1     2*256     0.4,-     32      2     58.09         59.38
Z2      30      256     -       256+128   0.4,-     32      5     59.98         61.63
Z2      30      256     -       2*256     0.4,-     32      7     60.75         62.62
Z2      30      256     0.1     2*256     0.2,-     32      7     59.86         60.82
Z2      10      512     0.5     2*128     -         16      7     55.63         55.90

Z2      10      128     0.5     4*32      -         32      9     56.77         58.55
Z2      10      256     0.5     4*64      -         32      6     56.75         58.80


>Issues to address
Tokenizer does not recognize De vocabs
  - fit_on_texts on both train(En) and test(De)
EmbLayer data imbalance
  - imbalanced En dev + imbalanced En/De test -> De prediction skews
  - balanced En dev + imbalanced En/De test -> De prediction skews
training only with En -> De embs not updated




>>Older results
>Experiments using CLARIN En imbalanced (12422 neg/23700 neu/13453 pos)
* best epoch num reported by Callback
* - means did not implement
setting MAXLEN  bilstm  lstm  dropout   dense   dropouts  batch   epoch F1-micro(En)  F1-micro(De)
M1      30      -       -     -         2*128   -         32      1     60.23         -
M1      30      256     -     0.9       2*128   -         32      1     60.62         -
M1      30      256     -     0.5       2*128   -         32      1     61.70         -
M1      30      256     -     0.2       2*128   -         32      1     61.48         -
M1      30      128     -     0.9       2*128   -         32      2     60.84         -
M1      30      128     -     0.5       2*128   -         32      1     60.67         -
M1      30      64      -     0.5       2*128   -         32      1     61.37         -
M1      30      64      -     0.5       2*256   -         32      1     59.90         -
M1      30      64      -     0.5       2*64    -         32      1     60.06         -
M1      30      32      -     0.5       2*32    -         32      1     61.22         -

M2      30      128     -     0.9       2*128   -         32      13    60.21         -
M2      30      128     -     0.5       2*128   -         32      14    60.39         -
M2      30      -       128   0.5       2*128   -         32      11    60.33         -
M2      30      -       64    0.5       2*64    -         32      13    59.15         -
M2      30      64      -     0.9       2*32    -         32      12    59.98         -
M2      30      64      -     0.9       2*32    -         128     15    58.32         -
M2      30      128     -     0.5       2*32    -         16      9     60.48         -
M2      30      128     -     0.5       2*64    -         16      11    59.94         -
M2      30      128     -     0.5       2*128   -         8       8     61.03         -
M2      30      128     -     0.2       2*128   -         8       4     60.56         -
M2      30      256     -     0.5       2*128   -         16      11    60.33         -
