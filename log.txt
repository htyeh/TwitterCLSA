>Embeddings
En-De: weighted averaging from https://github.com/pedrada88/crossembeddings-twitter

>Available datasets
EN: SemEval13-16 (7689 neg/22189 neu/19606 pos)
EN: SemEval13-16 balanced version
EN: CLARIN.si (12422 neg/23700 neu/13453 pos)
EN: CLARIN.si balanced version
DE: SB-10k (989 neg/4131 neu/1509 pos)
DE: CLARIN.si (13853 neg/42174 neu/19489 pos)

>Data split
for SemEval: predetermined 3000 dev/3000 test so far
for CLARIN: 70% train, take last 15% dev & 15% test
for CLARIN balanced: removed the last a few thousand Tweets from the balanced train/dev/test

>Tweet cleansing
1) removed Not Available ones
2) contractions are kept (own embeddings)
3) hashtags kept (might express sentiment)
4) all lowercase
5) removed @mentions/links
6) emoticon conversion (all mapped to English expressions as these words have embs in the joint BWE)
https://towardsdatascience.com/twitter-sentiment-analysis-using-fasttext-9ccd04465597
7) removed punctuation (after emoticon conversion they can be deleted also from between two words e.g. '...')

>Classical experiments (fine-tuning with De)
1) baseline: train/test on De data
2) fixed [EnDe BWE]
3) trainable [EnDe BWE]
4) combine fixed & trainable [EnDe BWE]

>Zero-shot experiments
* only En part of [EnDe BWE] get updates (no training on De vocab)
1) baseline: train/test on De data
2) fixed [En BWE], testing on De directly (sanity check for overlapping words)
3) *trainable [EnDe BWE]
4) *combine fixed & trainable [EnDe BWE]
5) update [En BWE], map to [De BWE]
6) possibly improving architecture (e.g. adding a Conv layer) to achieve better results

>Experiment settings (M-monolingual, C-classical transfer, Z-zero-shot transfer)
M1: En train (27000, val 0.1) / En test (3000), EmbLayer
M2: En train (27000, val 0.1) / En test (3000), En BWE fixed
M3: En train (27000, val 0.1) / En test (3000), En BWE trainable

>Experiments using CLARIN En imbalanced (12422 neg/23700 neu/13453 pos)
* best epoch # reported by Callback
setting MAXLEN  lstm    dropout dense     batch  epoch  F1(En)    F1(De)
M1      30      -       -       2*128     32      1     60.23     -
M1      30      256     0.9     2*128     32      1     60.62     -
M1      30      256     0.5     2*128     32      1     61.70     -
M1      30      256     0.2     2*128     32      1     61.48     -
M1      30      128     0.9     2*128     32      2     60.84     -
M1      30      128     0.5     2*128     32      1     60.67     -
M1      30      64      0.5     2*128     32      1     61.37     -
M1      30      64      0.5     2*256     32      1     59.90     -
M1      30      64      0.5     2*64      32      1     60.06     -

>Experiments using CLARIN En balanced (12422 neg/13000 neu/13453 pos)
setting MAXLEN  lstm    dropout dense     batch  epoch  F1(En)    F1(De)
M1      30      64      0.5     2*128     32      1     57.14     -
M1      30      128     0.5     2*128     32      1     60.71     -
M1      30      256     0.9     2*128     32      1     56.61     -
M1      30      256     0.5     2*128     32      1     59.03     -
M1      30      256     0.2     2*128     32      1     60.78     -
M1      30      128     0.5     2*256     32      1     59.81     -

>Experiments using SemEval En balanced
* best epoch # reported by Callback
setting MAXLEN  lstm    dropout dense     batch  epoch  F1(En)    F1(De)
M1      30      100     0.9     2*128     32      3     63.40     -
M1      30      100     0.9     2*64      32      3     63.31     -
M1      30      100     0.5     2*64      32      3     70.72     -
M2      30      100     0.5     2*64      32      5     63.96     -
M3      30      100     0.5     2*64      32      3     70.98     -
M3      30      100     0.5     2*128     32      5     70.76     -
M3      30      100     0.2     2*64      32      3     70.48     -


>Issues to address
1) Tokenizer does not recognize De vocabs
  - fit_on_texts on both train(En) and test(De)
2) training only with En -> De embs not updated
