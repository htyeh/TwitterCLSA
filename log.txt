>Embeddings
En-De: weighted averaging from https://github.com/pedrada88/crossembeddings-twitter

>Available datasets
EN: SemEval13-16 (7689 neg/22189 neu/19606 pos)
EN: SemEval13-16 balanced (10000 neg/10000 neu/ 10000 pos)
EN: CLARIN.si (12422 neg/23700 neu/13453 pos)
EN: CLARIN.si balanced (12422 neg/~13000 neu/13453 pos)
DE: SB-10k (989 neg/4131 neu/1509 pos)
DE: SB-10k balanced used for zero-shot testing (989 neg/~1500 neu/1509 pos)
DE: CLARIN.si (13853 neg/42174 neu/19489 pos)

>Datasets in use
Train: balanced CLARIN En 8696 neg/9000 neu/9415 pos
Dev: imbalanced CLARIN En 1863 neg/3555 neu/2019 pos
Test: imbalanced CLARIN En 1863 neg/3555 neu/2019 pos
Test: imbalanced De 989 neg/4131 neu/1509 pos

>Data split
70/15/15 (last 30% Tweets taken directly for dev & test)

>Tweet cleansing
1) removed Not Available ones
2) contractions are kept (own embeddings)
3) hashtags kept (might express sentiment)
4) all lowercase
5) removed @mentions/links
6) emoticon conversion (all mapped to English expressions as these words have embs in the joint BWE)
* Keras Tokenizer does not encode emoticons
https://towardsdatascience.com/twitter-sentiment-analysis-using-fasttext-9ccd04465597
7) removed punctuation (after emoticon conversion they can be deleted also from between two words e.g. '...')
8) converted \u2019 (to '), \u002c (to ,)

>Experiments
1) baseline: train/test on De data
2) fixed [En BWE], testing on De directly (sanity check for overlapping words)
2) fixed [EnDe BWE] (classical fine-tuning)
2) *fixed [EnDe BWE] (zero-shot)
3) *trainable [EnDe BWE] (classical fine-tuning)
3) trainable [EnDe BWE] (zero-shot)
4) combine fixed & trainable [EnDe BWE]
5) update [En BWE], map to [De BWE]
6) search for better architecture (e.g. adding a Conv layer) to achieve better results
* only En part of [EnDe BWE] get updates (no training on De vocab)

>Experiment settings (M-monolingual, C-classical transfer, Z-zero-shot transfer)
M1: En train/dev/test, EmbLayer
M2: En train/dev/test, [EnDe BWE] fixed
M3: En train/dev/test [EnDe BWE] trainable
Z1: En train/dev/test, EmbLayer; De test (zero-shot)
Z2: En train/dev/test, [EnDe BWE] fixed; De test (zero-shot)
Z3: M3: En train/dev/test [EnDe BWE] trainable; De test (zero-shot)

>Experiments using CLARIN En balanced (12422 neg/13000 neu/13453 pos)
* best epoch num reported by Callback
* - means did not implement
setting MAXLEN  bilstm  dropout dense     dropouts  batch  epoch  F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
Z1      30      32      -       2*64      -         32      1     57.46         62.80         57.94         46.96
Z1      30      64      -       2*64      -         32      1     54.50         60.28         54.71         48.84
Z1      30      64      -       2*128     -         32
Z1      30      128     -       2*128     -         32
Z1      10      64      0.5     2*32      -         8       1     57.75         ?
Z1      10      128     0.5     2*64      -         8       1     56.99         ?
Z1      10      128     0.5     2*128     -         16      1     57.72         ?
Z1      30      128     0.5     2*128     -         16      1     59.78         ?
Z1      30      256     0.5     256+128   -         16      1     58.13         ?
setting MAXLEN  bilstm  dropout dense     dropouts  batch  epoch  F1(En)    F1(De)
Z2      10      -       -       1*128     -         16      2     57.02     59.10
Z2      10      -       -       3*128     -         16      1     57.04     60.36
Z2      30      32      -       2*32      0.4,-     32      6     58.41     59.12
Z2      30      32      -       2*64      0.4,-     32      7     59.41     60.86
Z2      30      64      -       2*64      0.4,-     32      8     60.00     60.72
Z2      30      64      -       2*128     0.4,-     32      7     60.05     60.82
Z2      30      128     -       2*128     0.4,-     32      5     60.37     61.76
Z2      30      256     -       256+128   0.4,-     32      5     59.98     61.63
Z2      30      256     -       2*256     0.4,-     32      7     60.75     62.62
Z2      30      256     0.1     2*256     0.2,-     32      7     59.86     60.82

Z2      10      128     0.5     4*32      -         32      9     56.77     58.55
Z2      10      256     0.5     4*64      -         32      6     56.75     58.80
Z2      10      512     0.5     2*128     -         16      7     55.63     55.90
Z2      30      128     0.1     2*128     -,-       32      2     60.21     59.54

Z2      30      128     0.1     2*256     0.4,-     32      2     58.09     59.38
Z2      30      128     0.1     128+64    0.4,0.2   32      10    58.83     59.38
Z2      30      128     0.1     128+64    0.4,-     32      7     61.18     64.26
Z2      30      128     0.1     128+64    0.2,-     32      5     59.69     60.28
Z2      30      128     0.1     2*128     0.4,-     32      10    60.35     63.10
Z2      30      128     0.1     64+64     0.4,-     32      7     60.64     62.50


>Issues to address
1) Tokenizer does not recognize De vocabs
  - fit_on_texts on both train(En) and test(De)
2) training only with En -> De embs not updated




>>Older results
>Experiments using CLARIN En imbalanced (12422 neg/23700 neu/13453 pos)
* best epoch num reported by Callback
* - means did not implement
setting MAXLEN  bilstm  lstm  dropout   dense   dropouts  batch   epoch F1(En)    F1(De)
M1      30      -       -     -         2*128   -         32      1     60.23     -
M1      30      256     -     0.9       2*128   -         32      1     60.62     -
M1      30      256     -     0.5       2*128   -         32      1     61.70     -
M1      30      256     -     0.2       2*128   -         32      1     61.48     -
M1      30      128     -     0.9       2*128   -         32      2     60.84     -
M1      30      128     -     0.5       2*128   -         32      1     60.67     -
M1      30      64      -     0.5       2*128   -         32      1     61.37     -
M1      30      64      -     0.5       2*256   -         32      1     59.90     -
M1      30      64      -     0.5       2*64    -         32      1     60.06     -
M1      30      32      -     0.5       2*32    -         32      1     61.22     -

M2      30      128     -     0.9       2*128   -         32      13    60.21     -
M2      30      128     -     0.5       2*128   -         32      14    60.39     -
M2      30      -       128   0.5       2*128   -         32      11    60.33     -
M2      30      -       64    0.5       2*64    -         32      13    59.15     -
M2      30      64      -     0.9       2*32    -         32      12    59.98     -
M2      30      64      -     0.9       2*32    -         128     15    58.32     -
M2      30      128     -     0.5       2*32    -         16      9     60.48     -
M2      30      128     -     0.5       2*64    -         16      11    59.94     -
M2      30      128     -     0.5       2*128   -         8       8     61.03     -
M2      30      128     -     0.2       2*128   -         8       4     60.56     -
M2      30      256     -     0.5       2*128   -         16      11    60.33     -
