-------------------------------------------------------------------------------------------------------------------------------------------
EXPERIMENT SETTINGS IN THIS LOG
-------------------------------------------------------------------------------------------------------------------------------------------
(M-monolingual, C-classical transfer, Z-zero-shot transfer)
Z5: En train/dev/test, [En BWE]; map [De BWE] to [En BWE]; De test (zero-shot)

-----------------------------------------------------------------------------------------------------
INVESTIGATION OF TUNED-BWE -- AVG COSSIM
-----------------------------------------------------------------------------------------------------
** setting = Z3 (to investigate En embeddings)
** trained BWE is extracted
** original BWE is filtered for the same vocabs
** calculate avg. cos. similarity
** add instructions for cossim.py
** where this is useful: to maximize the effect of 'updated embs', we need to find a LR where orig. En BWE is maximally trained
Emb               LR        avg.cossim
Collados et al.   0.01      0.48
                  e-3       0.63
                  e-4       0.64
                  e-5       0.64
                  e-6       0.64

-------------------------------------------------------------------------------------------------------------------------------------------
MUSE & VECMAP: NOTES
-------------------------------------------------------------------------------------------------------------------------------------------
** train: En embeddings + En datasets, emb-LR=e-4
** save trained embeddings
** map De embeddings to En space (MUSE/VecMap)
** resulting En space much smaller than De because of corpora-vocab filtering
** consider two types of identical vocab merger: avg & keep-en
** evaluation on the merged EnDeBWE (refer to section 'INTRINSIC EVALUATION')

< implementation example >
** 53626 tokens found in En datasets
** 412776 De vocabs to map
** 24154 overlapping tokens
** 442248 embeddings in the merged BWE
** merged BWE fed to perform Z2/C2

< MUSE >
** parameter adjustments:
  * MUSE epoch_size 100000/500000 instead of default 1000000
  * MUSE emb_dim adapted to embeddings used
** python3 unsupervised.py --src_lang de --tgt_lang en --src_emb crosslingual_EN-DE_german_twitter_100d_weighted.txt.w2v --tgt_emb trained_en_embs.txt --n_refinement 5
** python3 supervised.py --src_lang de --tgt_lang en --src_emb crosslingual_EN-DE_german_twitter_100d_weighted.txt.w2v --tgt_emb trained_en_embs.txt --n_refinement 5 --dico_train default

< VecMap >

< comments >
** Z5 shows improvement upon ZM (w/o De embeddings), but underperforms other settings (Z2-4) except in the supervised setting
** it can be thus inferred that Z5 shows no significant purpose, but lowers En performance by averaging embs


-------------------------------------------------------------------------------------------------------------------------------------------
MUSE
-------------------------------------------------------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------------------------------------------------------
VECMAP - INTRODUCTION
-------------------------------------------------------------------------------------------------------------------------------------------
* unsupervised
python3 map_embeddings.py --unsupervised crosslingual_EN-DE_german_twitter_100d_weighted.txt.w2v vectors-en.txt de_unsupervised_mapped.txt en_unsupervised_mapped.txt

* identical (no seed-dict, use identical words)
python3 map_embeddings.py --identical crosslingual_EN-DE_german_twitter_100d_weighted.txt.w2v vectors-en.txt de_identical_mapped.txt en_identical_mapped.txt

* semi-supervised (small seed-dict)
python3 map_embeddings.py --semi_supervised MUSE_dictionaries/de-en.txt crosslingual_EN-DE_german_twitter_100d_weighted.txt.w2v vectors-en.txt de_supervised_mapped.txt en_supervised_mapped.txt

* supervised (seed-dict)
python3 map_embeddings.py --supervised MUSE_dictionaries/de-en.txt crosslingual_EN-DE_german_twitter_100d_weighted.txt.w2v vectors-en.txt de_supervised_mapped.txt en_supervised_mapped.txt


-------------------------------------------------------------------------------------------------------------------------------------------
INTRINSIC EVALUATION SCORES FROM MUSE
-------------------------------------------------------------------------------------------------------------------------------------------
                          De->En Conneau et al. |  De->En TwitterCLSA ----> epoch_size 100000->500000
-----------------------------------------------------------------------------------------------------
NN                        59.6                  |  35.74                |   29.87
CSLS                      66.4                  |  35.23                |   30.70
Refined NN                69.6                  |  34.40                |   34.56
Refined CSLS              72.2                  |  35.23                |   35.74
CLWordsim ADV             0.7                   |  0.63                 |   0.52
CLWordsim ADV Refined     0.71                  |  0.61                 |   0.62
-----------------------------------------------------------------------------------------------------
                          De->En Conneau et al. |  De->En Tw.CLSA superv.|  GloVe supervised
-----------------------------------------------------------------------------------------------------
Procrustes NN             67.7                  |  35.07                |   13.93
Procrustes CSLS           72.4                  |  35.07                |   12.08
CLWordsim                 0.72                  |  0.64                 |   0.54
-----------------------------------------------------------------------------------------------------
                          De->En Conneau et al. |  GloVe unsupervised   |
-----------------------------------------------------------------------------------------------------
NN                        59.6                  |  0                    |
CSLS                      66.4                  |  0                    |
Refined NN                69.6                  |  0                    |
Refined CSLS              72.2                  |  0                    |
CLWordsim ADV             0.7                   |  0.15                 |
CLWordsim ADV Refined     0.71                  |  -0.06                |

** comment: suggested by https://www.aclweb.org/anthology/P18-1073.pdf MUSE is unable to handle radically different spaces w/ unsupervised method
-------------------------------------------------------------------------------------------------------------------------------------------
SIDE EXPERIMENT: MAP DE -> UNTRAINED GLOVE TWITTER
-------------------------------------------------------------------------------------------------------------------------------------------
                          De->En Conneau et al.    |  De->GloVe side test (supervised)
Procrustes NN                67.7                  |  35.07
Procrustes CSLS              72.4                  |  34.73
CLWordsim                    0.72                  |  0.64


-------------------------------------------------------------------------------------------------------------------------------------------
OWN EVALUATION: cossim_crosslingual
-------------------------------------------------------------------------------------------------------------------------------------------
** en-de & de-en dict available from MUSE (101931 en-de pairs & 101997 de-en pairs) used for evaluation
** take average of en-de cossim & de-en cossim (cossim_crosslingual.py)
** python3 cossim_crosslingual.py en_embs.txt de_embs.txt -d MUSE_dictionaries/en-de.txt

-------------------------------------------------------------------------------------------------------------------------------------------
RESULTS
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> CLARIN.si full (train 8696/16590/9415; dev 1863/3555/2019; test 1863/3555/2019)
<DE> CLARIN.si 10% (CLARIN_small10) (train 970/2952/1365; dev 2078/6326/2923; test 2078/6326/2923)

                                            en-de_avg_cossim  de-en_avg_cossim  avg_cossim  merger  setting EnMicro DeMicro EnMacro DeMacro
Collados, no tuning                         0.7175            0.7124            0.7150      -       Z2      63.44   56.97   61.50   50.85
Collados, tuned & no mapping                0.6471            0.6397            0.6434      -       C2      61.52   60.03   58.94   51.87
-------------------------------------------------------------------------------------------------------------------------------------------
Collados, vecmap supervised                 0.6401            0.6425            0.6412      avg     Z2      60.58   54.04   59.28   48.72
dict=en-de.train.txt                                                                        keep    Z2      61.40   56.11   60.09   45.39
-------------------------------------------------------------------------------------------------------------------------------------------
Collados, vecmap supervised                 0.6603            0.6578            0.6591      avg     Z2      61.41   55.27   59.43   47.82
dict=MUSE de-en.txt                                                                         keep    Z2      60.16   56.30   56.75   48.75
-------------------------------------------------------------------------------------------------------------------------------------------
Collados, vecmap semi-supervised            0.6270            0.6246            0.6258      avg     Z2      60.84   52.07   58.28   47.77
dict=MUSE de-en.txt                                                                         keep    Z2      61.48   46.93   60.19   45.46
-------------------------------------------------------------------------------------------------------------------------------------------
Collados, vecmap identical                  0.6269            0.6246            0.6258      avg     Z2      61.23   51.94   59.03   48.19
                                                                                            keep    Z2      61.71   58.19   59.14   50.19
                                         -- add. test because De close to baseline Z2 --    keep    C2      60.25   59.01   58.65   51.91
-------------------------------------------------------------------------------------------------------------------------------------------
Collados, vecmap unsupervised               0.1158            0.1153            0.1156      avg     Z2      59.82   44.12   57.43   38.53
                                                                                            keep    Z2      60.86   42.37   60.08   35.59
-------------------------------------------------------------------------------------------------------------------------------------------
Collados, MUSE supervised                   0.6144            0.6128            0.6136      avg     Z2      61.56   52.38   60.07   49.91
dict=default                                                                                keep    Z2      61.52   54.90   59.21   51.20
-------------------------------------------------------------------------------------------------------------------------------------------
Collados, MUSE unsupervised                 0.6179            0.6148            0.6164      avg     Z2      61.39   51.96   59.86   49.61
epoch_size=100000                                                                           keep    Z2      61.72   53.85   59.81   50.65
-------------------------------------------------------------------------------------------------------------------------------------------
Collados, MUSE unsupervised                 0.6220            0.6185            0.6203      avg     Z2      61.29   51.88   60.07   49.89
epoch_size=500000                                                                           keep    Z2      61.96   53.55   60.48   50.02
-------------------------------------------------------------------------------------------------------------------------------------------
GloVe Twitter, no mapping                   -0.003            -0.004            -0.004      -       ZM      63.02   57.90   61.01   38.53
-------------------------------------------------------------------------------------------------------------------------------------------
GloVe Twitter, MUSE supervised              0.4084            0.4286            0.4185      avg     Z2      63.30   58.87   61.86   42.70
dict=default                                                                                keep    Z2      60.84   56.84   58.85   52.11
                                         -- add. test because De close to baseline Z2 --    keep    C2      57.62   57.80   55.06   49.84
-------------------------------------------------------------------------------------------------------------------------------------------
GloVe Twitter, MUSE unsupervised            0.0175            0.0215            0.0195      avg     Z2      62.87   56.80   61.58   32.99
epoch_size=100000                                                                           keep    Z2      46.91   52.75   37.77   34.11
-------------------------------------------------------------------------------------------------------------------------------------------




-------------------------------------------------------------------------------------------------------------------------------------------
SUMMARY OF ALL
-------------------------------------------------------------------------------------------------------------------------------------------
LOWER & HIGHER BOUNDS
-------------------------------------------------------------------------------------------------------------------------------------------
setting         F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
Z1                            54.78                       32.43
ZM                            54.08                       41.57
M2-De-full                    59.49                       54.13
C2-De-full                    60.04                       54.18


-------------------------------------------------------------------------------------------------------------------------------------------
OVERVIEW OF ALL SCORES
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> CLARIN.si full (train 8696/16590/9415; dev 1863/3555/2019; test 1863/3555/2019)
<DE> CLARIN.si 10% (CLARIN_small10) (train 970/2952/1365; dev 2078/6326/2923; test 2078/6326/2923)
setting MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
< ZERO-SHOT >
Z1      30      128     0.2     2*64      -         64    1     60.99         54.78         59.21         32.43
ZM      30      128     0.2     2*64      -         64    10    63.19         54.08         61.53         41.57
Z2      30      128     0.2     2*64      -         64    10    63.44         56.97         61.50         50.85
Z3      30      128     0.2     2*64      -         64    1     62.03         55.38         60.71         49.62
Z4      30      128     0.2     2*64      -         64    1     62.86         56.43         60.74         50.37
Z5      30      128     0.2     2*64      -         64    2     62.08         55.11         60.83         51.11
< CLASSICAL WITH 5287 DE >
C1      30      128     0.2     2*64      -         64    -     60.97         55.79         58.01         45.13
C2      30      128     0.2     2*64      -         64    -     61.52         60.03         58.94         51.87
C3      30      128     0.2     2*64      -         64    -     59.88         57.71         58.30         48.23
C4      30      128     0.2     2*64      -         64    -     61.82         57.45         60.88         50.11
CX      30      128     0.2     2*64      -         64    -     63.05         59.08         60.41         50.99
< CLASSICAL WITH 264 DE >
C1      30      128     0.2     2*64      -         64          60.26         55.48         56.98         31.96
C2      30      128     0.2     2*64      -         64          62.21         59.26         58.21         48.32
C3      30      128     0.2     2*64      -         64          61.84         58.67         59.06         46.96
C4      30      128     0.2     2*64      -         64          62.85         57.23         61.01         50.42
< EMB-ENHANCER ZERO-SHOT >                          * (LR) setting with best En scores is recorded
Z1      30      128     0.2     2*64      -         64    -     60.14         53.06         59.08         34.01
Z3      30      128     0.2     2*64      -         64    -     62.12         48.01         61.04         46.13
Z4      30      128     0.2     2*64      -         64    -     61.48         51.74         60.34         49.87
< EMB-ENHANCER CLASSICAL WITH 5287 DE >             * (LR) setting with best De scores is recorded
C1      30      128     0.2     2*64      -         64    -     57.89         51.80         56.47         39.43
C3      30      128     0.2     2*64      -         64    -     61.22         58.32         58.77         49.79
C4      30      128     0.2     2*64      -         64    -     61.51         57.81         60.06         51.04
< EMB-ENHANCER CLASSICAL WITH 264 DE >              * one (LR) setting tested for efficiency
C1      30      128     0.2     2*64      -         64    -     57.82         47.36         57.33         38.04
C3      30      128     0.2     2*64      -         64    -     61.10         53.68         60.23         49.92
C4      30      128     0.2     2*64      -         64    -     59.74         53.60         59.20         50.18
< ARCH-ENHANCER ZERO-SHOT >                         * one (LR) setting tested
Z3      30      128     0.2     2*64      -         64    -     64.48         56.10         63.11         50.42
Z4      30      128     0.2     2*64      -         64    -     64.05         57.41         62.72         51.02
< ARCH-ENHANCER CLASSICAL WITH 5287 DE >            * (LR) setting with best De scores is recorded
C3      30      128     0.2     2*64      -         64    -     62.29         60.07         61.09         52.83   > baseline (C2 60.03/51.87)
C4      30      128     0.2     2*64      -         64    -     62.88         59.97         61.32         51.96   ? > baseline (C2 60.03/51.87)
CX      30      128     0.2     2*64      -         64    -     63.09         59.46         61.42         52.25
< ARCH-ENHANCER CLASSICAL WITH 264 DE >             * one (LR) setting tested for efficiency
C3      30      128     0.2     2*64      -         64    -     63.42         59.64         60.68         49.22   about SOTA w/ 10572 De
C4      30      128     0.2     2*64      -         64    -     62.60         59.76         59.86         50.82   > baseline (C2 59.26/48.32)
