-------------------------------------------------------------------------------------------------------------------------------------------
EXPERIMENT SETTINGS IN THIS LOG
-------------------------------------------------------------------------------------------------------------------------------------------
** results recorded in this log:
  * investigate performance of GloVe Twitter embeddings
  * investigate effectiveness of pickle-loaded Tokenizer (outdated - loading Tokenizer no longer needed)
  * compare performance of fine-tuning against zero-shot and monolingual training
  * compare different fine-tuning data sizes and LR
  * investigate performance of combining fixed + trainable BWE
  * propose possibility of training both BWE and rest of the model

(M-monolingual, C-classical transfer, Z-zero-shot transfer)
M1: EmbLayer
M2: [EnDe BWE] fixed
M3: [EnDe BWE] trainable
Z1: En train/dev/test, EmbLayer; De test (zero-shot) -> De vocab should have random embeddings
ZM: En train/dev/test, [En BWE]; De test (zero-shot) -> De vocab should embeddings of 0's
Z2: En train/dev/test, [EnDe BWE] fixed; De test (zero-shot)
Z3: En train/dev/test, [EnDe BWE] trainable; De test (zero-shot)
Z4: En train/dev/test, [EnDe BWE fixed][EnDe BWE trainable]; De test (zero-shot)
C1: En train/dev/test, EmbLayer; De train/dev/test (fine-tuning)
C2: En train/dev/test, [EnDe BWE] fixed; De train/dev/test (fine-tuning)
C3: En train/dev/test, [EnDe BWE] trainable; De train/dev/test (fine-tuning)
C4: En train/dev/test, [EnDe BWE fixed][EnDe BWE trainable]; De train/dev/test (fine-tuning)


-------------------------------------------------------------------------------------------------------------------------------------------
TESTING EMBEDDINGS
-------------------------------------------------------------------------------------------------------------------------------------------
< tests to compare performance of embeddings: Collados et al. vs GloVe Twitter >
<EN> CLARIN full train (8696 neg/16590 neu/9415 pos)
<EN> CLARIN full dev (1863 neg/3555 neu/2019 pos)
<EN> CLARIN full test (1863 neg/3555 neu/2019 pos)
<DE> CLARIN full test (2078 neg/6326 neu/2923 pos)
<model architecture: bilstm(128) + dropout(0.2) + dense(64) + dense(64)> <batch=64> <setting=ZM>
embeddings        dim   epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)  pred. comment
EmbLayer(Z1)      100   1     60.65         65.44         59.39         44.94         De neu-skewed
Collados et al.   100   10    63.19         54.08         61.53         41.57
GloVe Twitter     100   5     62.66         58.43         61.53         38.50         De neu-skewed
GloVe Twitter     200   3     63.09         58.57         61.10         37.94         De neu-skewed


-------------------------------------------------------------------------------------------------------------------------------------------
PREPARATION FOR CLASSICAL TRANSFER SETTINGS
-------------------------------------------------------------------------------------------------------------------------------------------
< notes on fine-tuning >
** fine-tuning: try full De / downsized De training
** twnet_en trains with en / twnet_de trains with de / evaluation done with twnet_de
** Tokenizer from twnet_en saved with pickle / refitting Tokenizer in twnet_de would result in diff. indices and scores
** dumping with pickle preferred over json for simpler implementation and storage (about 6M vs 17M)
** Tokenizer is fit on all De train/dev/test in twnet_en so that in twnet_de all De vocab can be allocated embs

< steps of training + tuning >
** twnet_en: Tokenizer is fit on EN train/dev/test + DE train/dev/test
** model trained/val. on EN, record scores on EN and DE, save best model
** scores recorded under step "train" can be understood as zero-shot performance
** twnet_de: load best model, restore Tokenizer from pickle
** model trained/val. on DE, record scores on EN and DE, save best model
** scores recorded under step "tune" can be understood as fine-tuned performance
** twnet_final_check loads the final model and checks all scores
-------------------------------------------------------------------------------------------------------------------------------------------
< tests with restored pickle >
<model 1: Tokenizer on EN CLARIN train/dev/test and DE CLARIN test, setting=Z1>
<model 2: Tokenizer=loaded pickle from model 1>
<model 3: Tokenizer=loaded pickle from model 1>
                                                                F1-micro(1)   F1-micro(2)   F1-macro(1)   F1-macro(2)
<model 1: test on EN CLARIN (1) / DE CLARIN (2)>                61.62         55.39         58.29         30.44
<model 2: test on EN CLARIN (1) / DE CLARIN (2)>                61.62         55.39         58.29         30.44
<model 3: test on DE 10k (1) / DE CLARIN (2)>                   65.53         55.39         41.58         30.44
** observation1: restored Tokenizer + same test sets lead to same results (model 1 vs model 2)
** observation2: abnormal performance on DE 10k test data: better micro than when testing on EN (model 2 vs model 3)


-------------------------------------------------------------------------------------------------------------------------------------------
EVALUATING THE EFFECTS OF DE DATASET SIZE
-------------------------------------------------------------------------------------------------------------------------------------------
< classical settings, with full De data >
<EN> CLARIN.si full (train 8696/16590/9415; dev 1863/3555/2019; test 1863/3555/2019)
<DE> CLARIN.si full (train 9697/29522/13643; dev 2078/6326/2923; test 2078/6326/2923)
setting step  MAXLEN  bilstm  dropout dense     dropouts  batch epoch-en  epoch-de  F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
C1      train 30      128     0.2     2*64      -         64    1         -         60.93         53.98         59.32         33.26
        tune  30      128     0.2     2*64      -         64    -         1         60.56         57.43         57.34         51.52
C2      train 30      128     0.2     2*64      -         64    11        -         63.15         55.64         61.64         50.26
        tune  30      128     0.2     2*64      -         64    -         2         60.94         60.04         59.16         54.18
C3      train 30      128     0.2     2*64      -         64    1         -         62.57         55.53         61.02         49.29
        tune  30      128     0.2     2*64      -         64    -         1         59.98         57.87         59.29         53.05
** observation1: De performance comparable to those in De-monolingual settings after fine-tuning
** observation2: small decrease in En performance after fine-tuning
** comment on C3: BWE in C2 increases performance for En and De, updating BWE in C3 negative for both
-------------------------------------------------------------------------------------------------------------------------------------------
< as comparison: scores when train/dev/test with full De >
<DE> CLARIN.si full (train 9697/29522/13643; dev 2078/6326/2923; test 2078/6326/2923)
setting       MAXLEN  bilstm  dropout dense     dropouts  batch           epoch     F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
M1            30      128     0.2     2*64      -         64              1         -             57.89         -             49.35
M2            30      128     0.2     2*64      -         64              6         -             59.49         -             54.13
M3            30      128     0.2     2*64      -         64              1         -             58.76         -             52.50
-------------------------------------------------------------------------------------------------------------------------------------------
< as comparison: scores when train/dev/test with 20% De >
<DE> CLARIN.si 20% (CLARIN_small20) (train 1939/5904/2729; dev 2078/6326/2923; test 2078/6326/2923)
setting       MAXLEN  bilstm  dropout dense     dropouts  batch           epoch     F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
M1            30      128     0.2     2*64      -         64              1         -             55.26         -             41.55
M2            30      128     0.2     2*64      -         64              5         -             58.60         -             50.59
M3            30      128     0.2     2*64      -         64              1         -             57.23         -             48.82
** observation: still better scores than zero-shot
-------------------------------------------------------------------------------------------------------------------------------------------
< as comparison: scores when train/dev/test with 10% De >
<DE> CLARIN.si 10% (CLARIN_small10) (train 970/2952/1365; dev 2078/6326/2923; test 2078/6326/2923)
setting       MAXLEN  bilstm  dropout dense     dropouts  batch           epoch     F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
M1            30      128     0.2     2*64      -         64              1         -             54.25         -             28.21
M2            30      128     0.2     2*64      -         64              6         -             56.94         -             50.70
M3            30      128     0.2     2*64      -         64              1         -             54.24         -             38.22
** observation: lower scores as dataset size decreases, also overfitting to predicting majority class
** observation: possible overfitting & underrepresentation of minority classes in low-resource setting (ref. neu-skewness)
** observation: slightly better scores with zero-shot:
setting       MAXLEN  bilstm  dropout dense     dropouts  batch epoch               F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
Z1            30      128     0.2     2*64      -         64    1                   60.99         54.78         59.21         32.43
Z2            30      128     0.2     2*64      -         64    8                   63.13         56.71         61.31         50.85
Z3            30      128     0.2     2*64      -         64    1                   62.03         55.38         60.71         49.62
-------------------------------------------------------------------------------------------------------------------------------------------
< classical settings, with 10% De data >
<EN> CLARIN.si full (train 8696/16590/9415; dev 1863/3555/2019; test 1863/3555/2019)
<DE> CLARIN.si 10% (CLARIN_small10) (train 970/2952/1365; dev 2078/6326/2923; test 2078/6326/2923)
setting step  MAXLEN  bilstm  dropout dense     dropouts  batch epoch-en  epoch-de  F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
C1      train 30      128     0.2     2*64      -         64    1         -         61.29         54.28         59.09         32.60
        tune  30      128     0.2     2*64      -         64    -         1         60.97         55.79         58.01         45.13
C2      train 30      128     0.2     2*64      -         64    9         -         63.68         57.29         61.02         50.28
        tune  30      128     0.2     2*64      -         64    -         2         61.52         60.03         58.94         51.87
C3      train 30      128     0.2     2*64      -         64    1         -         62.96         57.08         61.29         49.56
        tune  30      128     0.2     2*64      -         64    -         1         60.25         56.69         57.63         46.63
-------------------------------------------------------------------------------------------------------------------------------------------
< fine-tuning compared with zero-shot scores >
<EN> CLARIN.si full (train 8696/16590/9415; dev 1863/3555/2019; test 1863/3555/2019)
<DE> CLARIN.si 10% (CLARIN_small10) (train 970/2952/1365; dev 2078/6326/2923; test 2078/6326/2923)
setting       MAXLEN  bilstm  dropout dense     dropouts  batch epoch               F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
ZM            30      128     0.2     2*64      -         64    10                  63.19         54.08         61.53         41.57
Z1            30      128     0.2     2*64      -         64    1                   60.99         54.78         59.21         32.43
C1      train 30      128     0.2     2*64      -         64    1         -         61.29         54.28         59.09         32.60
        tune  30      128     0.2     2*64      -         64    -         1         60.97         55.79         58.01         45.13

Z2            30      128     0.2     2*64      -         64    8                   63.13         56.71         61.31         50.85
C2      train 30      128     0.2     2*64      -         64    9         -         63.68         57.29         61.02         50.28
        tune  30      128     0.2     2*64      -         64    -         2         61.52         60.03         58.94         51.87

Z3            30      128     0.2     2*64      -         64    1                   62.03         55.38         60.71         49.62
C3      train 30      128     0.2     2*64      -         64    1         -         62.96         57.08         61.29         49.56
        tune  30      128     0.2     2*64      -         64    -         1         60.25         56.69         57.63         46.63


-------------------------------------------------------------------------------------------------------------------------------------------
SIZE OF FINE-TUNING DATASET VS PERFORMANCE
-------------------------------------------------------------------------------------------------------------------------------------------
< fine-tuning with full DE CLARIN >
setting step  MAXLEN  bilstm  dropout dense     dropouts  batch epoch-en  epoch-de  F1-micro(En)  F1-micro(De)  F1-macro(En) macro(De)
C1      train 30      128     0.2     2*64      -         64    1         -         60.93         53.98         59.32         33.26
        tune  30      128     0.2     2*64      -         64    -         1         60.56         57.43         57.34         51.52
C2      train 30      128     0.2     2*64      -         64    11        -         63.15         55.64         61.64         50.26
        tune  30      128     0.2     2*64      -         64    -         2         60.94         60.04         59.16         54.18
C3      train 30      128     0.2     2*64      -         64    1         -         62.57         55.53         61.02         49.29
        tune  30      128     0.2     2*64      -         64    -         1         59.98         57.87         59.29         53.05
< fine-tuning with 10% DE CLARIN >
setting step  MAXLEN  bilstm  dropout dense     dropouts  batch epoch-en  epoch-de  F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
C1      train 30      128     0.2     2*64      -         64    1         -         61.29         54.28         59.09         32.60
        tune  30      128     0.2     2*64      -         64    -         1         60.97         55.79         58.01         45.13
C2      train 30      128     0.2     2*64      -         64    9         -         63.68         57.29         61.02         50.28
        tune  30      128     0.2     2*64      -         64    -         2         61.52         60.03         58.94         51.87
C3      train 30      128     0.2     2*64      -         64    1         -         62.96         57.08         61.29         49.56
        tune  30      128     0.2     2*64      -         64    -         1         60.25         56.69         57.63         46.63
C4      train 30      128     0.2     2*64      -         64    1         -         62.87         54.38         61.83         50.48
        tune  30      128     0.2     2*64      -         64    -         1         61.82         57.45         60.88         50.11


-------------------------------------------------------------------------------------------------------------------------------------------
LEARNING RATE ADJUSTMENT IN FINE-TUNING
-------------------------------------------------------------------------------------------------------------------------------------------
< Adam default=0.001 >
<EN> CLARIN.si full (train 8696/16590/9415; dev 1863/3555/2019; test 1863/3555/2019)
<DE> CLARIN.si 10% (CLARIN_small10) (train 970/2952/1365; dev 2078/6326/2923; test 2078/6326/2923)
setting LR      MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
C2      0.01    30      128     0.2     2*64      -         64    1     59.27         60.43         55.16         48.62
C2      0.001   30      128     0.2     2*64      -         64    3     62.07         59.42         60.76         52.22
C2      0.0001  30      128     0.2     2*64      -         64    2     62.09         59.22         60.89         52.16
C2      e-5     30      128     0.2     2*64      -         64    1     62.15         59.21         60.94         52.14
C2      e-6     30      128     0.2     2*64      -         64    1     62.15         59.19         60.94         52.12
** observation: LR lower than default causes slower forgetting/learning for EN/DE


-------------------------------------------------------------------------------------------------------------------------------------------
FIXED + TRAINED EnDeBWE (Z4 & C4)
-------------------------------------------------------------------------------------------------------------------------------------------
< notes >
** input Tweet is passed into 2 separate EmbLayers, one of them updated during training
** BiLSTM input mismatch when using 200-dim merged BWE in Sequential is solved by applying sep. EmbLayers in the functional API
** C4 would use the 10% De Tweets to fine-tune
-------------------------------------------------------------------------------------------------------------------------------------------
< without loading pre-trained model >
<EN> CLARIN.si full (train 8696/16590/9415; dev 1863/3555/2019; test 1863/3555/2019)
<DE> CLARIN.si 10% (CLARIN_small10) (train 970/2952/1365; dev 2078/6326/2923; test 2078/6326/2923)
setting       MAXLEN  bilstm  dropout dense     dropouts  batch epoch               F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
Z4            30      128     0.2     2*64      -         64    1                   62.45         55.86         60.54         50.56
setting step  MAXLEN  bilstm  dropout dense     dropouts  batch epoch-en  epoch-de  F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
C4      train 30      128     0.2     2*64      -         64    1         -         62.87         54.38         61.83         50.48
        tune  30      128     0.2     2*64      -         64    -         1         61.82         57.45         60.88         50.11
-------------------------------------------------------------------------------------------------------------------------------------------
< Z4 comparison to Z2 & Z3 >
<EN> CLARIN.si full (train 8696/16590/9415; dev 1863/3555/2019; test 1863/3555/2019)
<DE> CLARIN.si 10% (CLARIN_small10) (train 970/2952/1365; dev 2078/6326/2923; test 2078/6326/2923)
setting MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
Z2      30      128     0.2     2*64      -         64    8     63.13         56.71         61.31         50.85
Z3      30      128     0.2     2*64      -         64    1     62.03         55.38         60.71         49.62
Z4      30      128     0.2     2*64      -         64    1     62.45         55.86         60.54         50.56
** observation: sentiment-tuning train-vocab BWE does not result in better performance
-------------------------------------------------------------------------------------------------------------------------------------------
< Z4 comparison to Z2 & Z3 >
see above section SIZE OF FINE-TUNING DATASET VS PERFORMANCE
** observation: like in Z settings sentiment-tuned BWE does not show clear improvements
-------------------------------------------------------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------------------------------------------------------
EMB/ARCHITECTURE JOINT OPTIMIZATION
-------------------------------------------------------------------------------------------------------------------------------------------
< effect of LR on optimizing embeddings >
< adjusting Adam LR for optimizing BWE >
< Adam default=0.001 >
<EN> CLARIN.si full (train 8696/16590/9415; dev 1863/3555/2019; test 1863/3555/2019)
setting LR      MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
Z3      default 30      128     0.2     2*64      -         64    1     62.03         55.38         60.71         49.62
Z3      0.0001  30      128     0.2     2*64      -         64    2     62.40         53.14         61.15         50.40
Z3      0.00001 30      128     0.2     2*64      -         64    12    60.74         52.44         59.30         49.31
** comment: setting LR to 0.0001 and tune BWE before freezing the layer for architecture training
-------------------------------------------------------------------------------------------------------------------------------------------
< experiments with EmbEnhancer: notes >
** when embs are updated, their weights are optimized after only 1 epoch, this can cause the rest of the model to be undertrained
** alternative 1 (EmbEnhancer): update embs, load model to train architecture further
  ** model1: the EmbLayer is trained while EarlyStopping patience set to 0
  ** model2: EmbLayer is frozen and architecture is trained
** alternative 2 (ArchEnhancer): train with EmbLayer fixed, load model to tune embs further
  ** model1: architecture trained with EmbLayer frozen
  ** model2: unfreeze EmbLayer and tune model (freeze other layers?)
** Enh.settings should be compared with corresponding 'fixed EmbLayer' settings, e.g. Z2
-------------------------------------------------------------------------------------------------------------------------------------------
< experiments with EmbEnhancer compared with same settings w/o Enhancer >
** note: embs tuned w/ LR lowered (e-4), for architecture training default e-3 & e-4 LR are tried out
setting LR(emb) LR(arch)  MAXLEN  bilstm  dropout dense     dropouts  bat.  epo(emb)  epo(arch) En-micro  De-micro  En-macro  De-macro
Z1      -       e-3       30      128     0.2     2*64      -         64    -         1         60.99     54.78     59.21     32.43
Z2      -       e-3       30      128     0.2     2*64      -         64    -         8         63.13     56.71     61.31     50.85
Enh.Z1  e-4     e-3       30      128     0.2     2*64      -         64    3         2         59.75     49.61     58.27     36.17
Enh.Z1  e-4     e-4       30      128     0.2     2*64      -         64    2         4         60.14     53.06     59.08     34.01

Z3      -       e-3       30      128     0.2     2*64      -         64    -         1         62.03     55.38     60.71     49.62
Z2      -       e-3       30      128     0.2     2*64      -         64    -         8         63.13     56.71     61.31     50.85
Enh.Z3  e-4     e-3       30      128     0.2     2*64      -         64    2         2         61.53     48.62     60.47     47.08
Enh.Z3  e-4     e-4       30      128     0.2     2*64      -         64    2         4         62.12     48.01     61.04     46.13

Z4      -       e-3       30      128     0.2     2*64      -         64    -         1         62.45     55.86     60.54     50.56
Z2      -       e-3       30      128     0.2     2*64      -         64    -         8         63.13     56.71     61.31     50.85
Enh.Z4  e-4     e-3       30      128     0.2     2*64      -         64    2         2         60.56     48.43     59.99     47.12
Enh.Z4  e-4     e-4       30      128     0.2     2*64      -         64    2         3         61.48     51.74     60.34     49.87

** observation: if the hypothesis 'trainable EmbLayer is NOT beneficial' is correct, Enh. should perform better than corresponding 'trainable EmbLayer' setting and worse than corresponding 'fixed EmbLayer' (Z2) setting
** observation: Enh. versions of Z1/Z3/Z4 (meaning: tuned embs + trained with EmbLayer fixed) perform worse than Z2 (fixed EmbLayer)
** observation: Enh. versions of Z1/Z3/Z4 also worse than original Z1/Z3/Z4, this is either irrelevant as Enh. versions are trained for 1 more epoch only, or the trained embs affect the ability of the rest layers to learn
-------------------------------------------------------------------------------------------------------------------------------------------
< experiments with ArchEnhancer: notes >
** with the tuned embs, architectures in EmbEnhacer seem to train for few epochs
** architecture trained with default e-3 LR, embs tuned w/ LR lowered (e-4)
** in the emb-tuning step all other layers are frozen
** in classical transfer settings: De data used to first tune architecture, then embs, same as in En part
-------------------------------------------------------------------------------------------------------------------------------------------
< experiments with ArchEnhancer compared with same settings w/o Enhancer >
setting LR(arch)  LR(emb) MAXLEN  bilstm  dropout dense     dropouts  bat.  epo(arch) epo(emb) En-micro  De-micro  En-macro  De-macro
Z3      e-3       -       30      128     0.2     2*64      -         64    1         -         62.03     55.38     60.71     49.62
Z2      e-3       -       30      128     0.2     2*64      -         64    8         -         63.13     56.71     61.31     50.85
Enh.Z3  e-3       e-4     30      128     0.2     2*64      -         64    7         1         64.48     56.10     63.11     50.42
*Enh.Z3 e-3       e-4     30      128     0.2     2*64      -         64    10        1         64.01     56.28     62.72     50.53
* w/o rest of layers frozen

Z4      e-3       -       30      128     0.2     2*64      -         64    1         -         62.45     55.86     60.54     50.56
Z2      e-3       -       30      128     0.2     2*64      -         64    8         -         63.13     56.71     61.31     50.85
Enh.Z4  e-3       e-4     30      128     0.2     2*64      -         64    8         3         64.05     57.41     62.72     51.02

** observation: contrary to what EmbEnhancer seems to prove, tuned embs in this setting improves performance (comp. Z2 with Enh.Z3/Z4)
** observation: ArchEnhancer > fixed BWE > trainable BWE > EmbEnhacer
** hypothesis: if tuned beforehand, sentiment-embs limit the capacity of arch. training and thus worsen the performance
** hypothesis: problems with tuning embs with fixed architecture?

setting step  MAXLEN  bilstm  dropout dense     dropouts  batch epo-en(arch,emb)  epo-de(arch,emb)  En-micro  De-micro  En-macro  De-macro
C3      train 30      128     0.2     2*64      -         64    1,-               -,-               62.96     57.08     61.29     49.56
        tune  30      128     0.2     2*64      -         64    -,-               1,-               60.25     56.69     57.63     46.63
C2      train 30      128     0.2     2*64      -         64    9,-               -,-               63.68     57.29     61.02     50.28
        tune  30      128     0.2     2*64      -         64    -,-               2,-               61.52     60.03     58.94     51.87
Enh.C3  train 30      128     0.2     2*64      -         64    9,2               -,-               64.34     55.94     63.05     50.25
        tune  30      128     0.2     2*64      -         64    -,-               2,1               62.29     60.07     61.09     52.83

C4      train 30      128     0.2     2*64      -         64    1,-               -,-               62.87     54.38     61.83     50.48
        tune  30      128     0.2     2*64      -         64    -,-               1,-               61.82     57.45     60.88     50.11
C2      train 30      128     0.2     2*64      -         64    9,-               -,-               63.68     57.29     61.02     50.28
        tune  30      128     0.2     2*64      -         64    -,-               2,-               61.52     60.03     58.94     51.87
C3      train 30      128     0.2     2*64      -         64    1,-               -,-               62.96     57.08     61.29     49.56
        tune  30      128     0.2     2*64      -         64    -,-               1,-               60.25     56.69     57.63     46.63
Enh.C4  train 30      128     0.2     2*64      -         64    8,2               -,-               64.20     55.82     62.96     50.03
        tune  30      128     0.2     2*64      -         64    -,-               1,1               62.88     59.97     61.32     51.96

** observation: Enh.C3 outperforms setting C2 w/ fixed BWE, C3 w/ trainable BWE
** observation: Enh.C4 outperforms original C4, C2 w/ fixed BWE, C3 w/ trainable BWE
