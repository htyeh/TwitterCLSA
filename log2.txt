-------------------------------------------------------------------------------------------------------------------------------------------
NOTES
-------------------------------------------------------------------------------------------------------------------------------------------
< fine-tuning >
** fine-tuning: try full De / downsized De training
** twnet_en trains with en / twnet_de trains with de / evaluation done with twnet_de
** Tokenizer from twnet_en saved with pickle / refitting Tokenizer in twnet_de would result in diff. indices and scores
** dumping with pickle preferred over json for simpler implementation and storage (about 6M vs 17M)
** Tokenizer is fit on all De train/dev/test in twnet_en so that in twnet_de all De vocab can be allocated embs

< steps of training + tuning >
** twnet_en: Tokenizer is fit on EN train/dev/test + DE train/dev/test
** model trained/val. on EN, record scores on EN and DE, save best model
** scores recorded under step "train" can be understood as zero-shot performance
** twnet_de: load best model, restore Tokenizer from pickle
** model trained/val. on DE, record scores on EN and DE, save best model
** scores recorded under step "tune" can be understood as fine-tuned performance
** twnet_final_check loads the final model and checks all scores

< EmbEnhancer >
** when embs are updated, their weights are optimized after only 1 epoch, this can cause the rest of the model to be undertrained
** alternative 1 (EmbEnhancer): update embs, load model to train architecture further
  ** model1: the EmbLayer is trained while EarlyStopping patience set to 0
  ** model2: EmbLayer is frozen and architecture is trained
** alternative 2 (ArchEnhancer): train with EmbLayer fixed, load model to tune embs further
  ** model1: architecture trained with EmbLayer frozen
  ** model2: unfreeze EmbLayer and tune model (freeze other layers?)

< ArchEnhancer >
** with the tuned embs, architectures in EmbEnhacer seem to train for few epochs
** architecture trained with default e-3 LR, embs tuned w/ LR lowered (e-4)
** in the emb-tuning step all other layers are frozen
** in classical transfer settings: De data used to first tune architecture, then embs, same as for En

-------------------------------------------------------------------------------------------------------------------------------------------
PICKLE
-------------------------------------------------------------------------------------------------------------------------------------------
<model 1: Tokenizer on EN CLARIN train/dev/test and DE CLARIN test, setting=Z1>
<model 2: Tokenizer=loaded pickle from model 1>
<model 3: Tokenizer=loaded pickle from model 1>
                                                                F1-micro(1)   F1-micro(2)   F1-macro(1)   F1-macro(2)
<model 1: test on EN CLARIN (1) / DE CLARIN (2)>                61.62         55.39         58.29         30.44
<model 2: test on EN CLARIN (1) / DE CLARIN (2)>                61.62         55.39         58.29         30.44
<model 3: test on DE 10k (1) / DE CLARIN (2)>                   65.53         55.39         41.58         30.44
** observation1: restored Tokenizer + same test sets lead to same results (model 1 vs model 2)
** observation2: abnormal performance on DE 10k test data: better micro than when testing on EN (model 2 vs model 3)


-------------------------------------------------------------------------------------------------------------------------------------------
DE FULL
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> CLARIN.si full (train 8696/16590/9415; dev 1863/3555/2019; test 1863/3555/2019)
<DE> CLARIN.si full (train 9697/29522/13643; dev 2078/6326/2923; test 2078/6326/2923)
< setting 1 >
setting       MAXLEN  bilstm  dropout dense     dropouts  batch           epoch     F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
M1            30      128     0.2     2*64      -         64              1         -             57.89         -             49.35
Z1            30      128     0.2     2*64      -         64              1         60.99         54.78         59.21         32.43
C1            30      128     0.2     2*64      -         64              1         60.56         57.43         57.34         51.52

< setting 2 >
setting       MAXLEN  bilstm  dropout dense     dropouts  batch           epoch     F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
M2            30      128     0.2     2*64      -         64              6         -             59.49         -             54.13
Z2            30      128     0.2     2*64      -         64              10        63.44         56.97         61.50         50.85
C2            30      128     0.2     2*64      -         64              2         60.94         60.04         59.16         54.18

< setting 3 >
setting       MAXLEN  bilstm  dropout dense     dropouts  batch           epoch     F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
M3            30      128     0.2     2*64      -         64              1         -             58.76         -             52.50
Z3            30      128     0.2     2*64      -         64              1         62.03         55.38         60.71         49.62
C3            30      128     0.2     2*64      -         64              1         59.98         57.87         59.29         53.05

< setting 4 >
setting       MAXLEN  bilstm  dropout dense     dropouts  batch           epoch     F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
M4            30      128     0.2     2*64      -         64              1         -             58.66         -             53.62
Z4            30      128     0.2     2*64      -         64              1         62.86         56.43         60.74         50.37
C4            30      128     0.2     2*64      -         64              1         60.63         59.16         59.75         54.60

-------------------------------------------------------------------------------------------------------------------------------------------
DE 20%
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> CLARIN.si full (train 8696/16590/9415; dev 1863/3555/2019; test 1863/3555/2019)
<DE> CLARIN.si 20% (CLARIN_small20) (train 1939/5904/2729; dev 2078/6326/2923; test 2078/6326/2923)

< setting 1 >
setting       MAXLEN  bilstm  dropout dense     dropouts  batch           epoch     F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
M1            30      128     0.2     2*64      -         64              1         -             55.26         -             41.55
Z1            30      128     0.2     2*64      -         64              1         60.99         54.78         59.21         32.43

< setting 2 >
setting       MAXLEN  bilstm  dropout dense     dropouts  batch           epoch     F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
M2            30      128     0.2     2*64      -         64              5         -             58.60         -             50.59
Z2            30      128     0.2     2*64      -         64              10        63.44         56.97         61.50         50.85

< setting 3 >
setting       MAXLEN  bilstm  dropout dense     dropouts  batch           epoch     F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
M3            30      128     0.2     2*64      -         64              1         -             57.23         -             48.82
Z3            30      128     0.2     2*64      -         64              1         62.03         55.38         60.71         49.62

< setting 4 >
setting       MAXLEN  bilstm  dropout dense     dropouts  batch           epoch     F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
M4            30      128     0.2     2*64      -         64              1         -             56.83         -             48.58
Z4            30      128     0.2     2*64      -         64              1         62.86         56.43         60.74         50.37

** observation: still better scores than zero-shot

-------------------------------------------------------------------------------------------------------------------------------------------
DE 10%
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> CLARIN.si full (train 8696/16590/9415; dev 1863/3555/2019; test 1863/3555/2019)
<DE> CLARIN.si 10% (train 970/2952/1365; dev 2078/6326/2923; test 2078/6326/2923)

< setting 1 >
setting       MAXLEN  bilstm  dropout dense     dropouts  batch           epoch     F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
M1            30      128     0.2     2*64      -         64              1         -             54.25         -             28.21
Z1            30      128     0.2     2*64      -         64              1         60.99         54.78         59.21         32.43
C1            30      128     0.2     2*64      -         64              1         60.97         55.79         58.01         45.13

< setting 2 >
setting       MAXLEN  bilstm  dropout dense     dropouts  batch           epoch     F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
M2            30      128     0.2     2*64      -         64              6         -             56.94         -             50.70
Z2            30      128     0.2     2*64      -         64              10        63.44         56.97         61.50         50.85
C2            30      128     0.2     2*64      -         64              2         61.52         60.03         58.94         51.87

< setting 3 >
setting       MAXLEN  bilstm  dropout dense     dropouts  batch           epoch     F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
M3            30      128     0.2     2*64      -         64              1         -             54.24         -             38.22
Z3            30      128     0.2     2*64      -         64              1         62.03         55.38         60.71         49.62
C3            30      128     0.2     2*64      -         64              1         59.88         57.71         58.30         48.23

< setting 4 >
setting       MAXLEN  bilstm  dropout dense     dropouts  batch           epoch     F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
M4            30      128     0.2     2*64      -         64              1         -             56.37         -             40.36
Z4            30      128     0.2     2*64      -         64              1         62.86         56.43         60.74         50.37
C4            30      128     0.2     2*64      -         64              1         61.82         57.45         60.88         50.11

** possible overfitting & underrepresentation of minority classes in low-resource setting (neu-skewness)
** slightly worse than zero-shot


-------------------------------------------------------------------------------------------------------------------------------------------
DE 0.5%
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> CLARIN.si full (train 8696/16590/9415; dev 1863/3555/2019; test 1863/3555/2019)
<DE> CLARIN.si 0.5% (train 48/148/68; dev 2078/6326/2923; test 2078/6326/2923)

< setting 1 >
setting       MAXLEN  bilstm  dropout dense     dropouts  batch           epoch     F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
M1            30      128     0.2     2*64      -         64              5         -             55.85         -             23.89
Z1            30      128     0.2     2*64      -         64              1         60.99         54.78         59.21         32.43
C1            30      128     0.2     2*64      -         64              1         60.26         55.48         56.98         31.96

< setting 2 >
setting       MAXLEN  bilstm  dropout dense     dropouts  batch           epoch     F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
M2            30      128     0.2     2*64      -         64              6         -             55.85         -             23.89
Z2            30      128     0.2     2*64      -         64              10        63.44         56.97         61.50         50.85
C2            30      128     0.2     2*64      -         64              2         62.21         59.26         58.21         48.32

< setting 3 >
setting       MAXLEN  bilstm  dropout dense     dropouts  batch           epoch     F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
M3            30      128     0.2     2*64      -         64              5         -             55.85         -             23.89
Z3            30      128     0.2     2*64      -         64              1         62.03         55.38         60.71         49.62
C3            30      128     0.2     2*64      -         64              1         61.84         58.67         59.06         46.96

< setting 4 >
setting       MAXLEN  bilstm  dropout dense     dropouts  batch           epoch     F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
M4            30      128     0.2     2*64      -         64              6         -             55.51         -             27.31
Z4            30      128     0.2     2*64      -         64              1         62.86         56.43         60.74         50.37
C4            30      128     0.2     2*64      -         64              1         62.85         57.23         61.01         50.42


-------------------------------------------------------------------------------------------------------------------------------------------
LEARNING RATE ADJUSTMENT IN FINE-TUNING
-------------------------------------------------------------------------------------------------------------------------------------------
LaTex: No
< Adam default=0.001 >
<EN> CLARIN.si full (train 8696/16590/9415; dev 1863/3555/2019; test 1863/3555/2019)
<DE> CLARIN.si 10% (CLARIN_small10) (train 970/2952/1365; dev 2078/6326/2923; test 2078/6326/2923)

setting LR      MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
C2      0.01    30      128     0.2     2*64      -         64    1     59.27         60.43         55.16         48.62
C2      0.001   30      128     0.2     2*64      -         64    3     62.07         59.42         60.76         52.22
C2      0.0001  30      128     0.2     2*64      -         64    2     62.09         59.22         60.89         52.16
C2      e-5     30      128     0.2     2*64      -         64    1     62.15         59.21         60.94         52.14
C2      e-6     30      128     0.2     2*64      -         64    1     62.15         59.19         60.94         52.12
** if not otherwise specified, LR applies for both train & tune


-------------------------------------------------------------------------------------------------------------------------------------------
HYPERPARAM. SEARCH FOR EMB/ARCH JOINT OPTIMIZATION
-------------------------------------------------------------------------------------------------------------------------------------------
LaTex: Yes
< effect of LR on optimizing embeddings >
< Adam default=0.001 >
<EN> CLARIN.si full (train 8696/16590/9415; dev 1863/3555/2019; test 1863/3555/2019)

setting LR      MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)  avg.cossim
Z3      0.01    30      128     0.2     2*64      -         64    1     61.01         52.10         58.66         37.20         0.48
Z3      default 30      128     0.2     2*64      -         64    1     62.03         55.38         60.71         49.62         0.63
Z3      e-4     30      128     0.2     2*64      -         64    2     62.40         53.14         61.15         50.40         0.64
Z3      e-5     30      128     0.2     2*64      -         64    12    60.74         52.44         59.30         49.31         0.64
Z3      e-6     30      128     0.2     2*64      -         64    148   60.99         50.67         59.01         47.11         0.64

** comment: setting LR to 0.0001 and tune BWE before freezing the layer for architecture training
** cossim: between trained space and original BWE, see section in log3
-------------------------------------------------------------------------------------------------------------------------------------------
LaTex: Yes
< effect of LR on optimizing architecture >
< Adam default=0.001 >
<EN> CLARIN.si full (train 8696/16590/9415; dev 1863/3555/2019; test 1863/3555/2019)
<DE> CLARIN.si 10% (CLARIN_small10) (train 970/2952/1365; dev 2078/6326/2923; test 2078/6326/2923)

setting LR      MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
Z2      0.1     30      128     0.2     2*64      -         64    3     47.80         55.85         21.56         23.89
Z2      0.01    30      128     0.2     2*64      -         64    3     63.41         58.90         61.35         52.51
Z2      0.002   30      128     0.2     2*64      -         64    8     63.44         57.74         61.58         51.04
Z2      default 30      128     0.2     2*64      -         64    10    63.44         56.97         61.50         50.85
Z2      5e-4    30      128     0.2     2*64      -         64    13    62.97         55.95         61.03         49.85
Z2      e-4     30      128     0.2     2*64      -         64    22    62.00         56.07         59.57         50.13
Z2      e-5     30      128     0.2     2*64      -         64    67    61.11         55.33         58.65         51.14

< keeping default LR in train-step >
C2      0.1     30      128     0.2     2*64      -         64    5     47.80         55.85         21.56         23.89
C2      0.01    30      128     0.2     2*64      -         64    2     59.44         59.74         57.60         52.46
C2      0.002   30      128     0.2     2*64      -         64    1     61.15         58.87         59.48         51.44
C2      default 30      128     0.2     2*64      -         64    2     61.52         60.03         58.94         51.87
C2      5e-4    30      128     0.2     2*64      -         64    3     62.03         59.54         60.18         51.58
C2      e-4     30      128     0.2     2*64      -         64    6     62.00         59.09         60.57         51.85
C2      e-5     30      128     0.2     2*64      -         64    30    62.84         58.77         60.78         50.89


-------------------------------------------------------------------------------------------------------------------------------------------
EMB-ENHANCER DE 10%
-------------------------------------------------------------------------------------------------------------------------------------------
LaTex: Yes

< baseline >
setting LR(emb) LR(arch)        epo(emb,arch)                       En-micro  De-micro  En-macro  De-macro
Z2      -       e-3             -,10                                63.44     56.97     61.50     50.85
C2      -       e-3             -,2                                 61.52     60.03     58.94     51.87

< setting 1 >
setting LR(emb) LR(arch)        epo(emb,arch)                       En-micro  De-micro  En-macro  De-macro
Z1      -       e-3             -,1                                 60.99     54.78     59.21     32.43
Enh.Z1  e-4     e-3             3,2                                 59.75     49.61     58.27     36.17
Enh.Z1  e-4     e-4             2,4                                 60.14     53.06     59.08     34.01
C1      -       e-3             -,1                                 60.97     55.79     58.01     45.13
Enh.C1  e-4     e-3             1,2                                 57.89     51.80     56.47     39.43
Enh.C1  e-4     e-2             1,1                                 55.99     55.00     48.64     33.90

< setting 3 >
setting LR(emb) LR(arch)        epo(emb,arch)                       En-micro  De-micro  En-macro  De-macro
Z3      -       e-3             -,1                                 62.03     55.38     60.71     49.62
Enh.Z3  e-4     e-3             2,2                                 61.53     48.62     60.47     47.08
Enh.Z3  e-4     e-4             2,4                                 62.12     48.01     61.04     46.13
C3      -       e-3             -,1                                 59.88     57.71     58.30     48.23
Enh.C3  e-4     e-4             1,10                                61.38     56.57     59.97     50.27
Enh.C3  e-4     e-3             1,2                                 61.22     58.32     58.77     49.79
Enh.C3  e-4     e-2             1,2                                 59.28     58.94     55.95     48.80

< setting 4 >
setting LR(emb) LR(arch)        epo(emb,arch)                       En-micro  De-micro  En-macro  De-macro
Z4      -       e-3             -,1                                 62.86     56.43     60.74     50.37
Enh.Z4  e-4     e-3             2,2                                 60.56     48.43     59.99     47.12
Enh.Z4  e-4     e-4             2,3                                 61.48     51.74     60.34     49.87
C4      -       e-3             -,1                                 61.82     57.45     60.88     50.11
Enh.C4  e-4     e-4             1,4                                 61.51     57.81     60.06     51.04
Enh.C4  e-4     e-3             1,1                                 59.97     57.35     58.75     50.64
Enh.C4  e-4     e-2             1,5                                 55.63     55.28     54.18     49.58


-------------------------------------------------------------------------------------------------------------------------------------------
EMB-ENHANCER DE 0.5%
-------------------------------------------------------------------------------------------------------------------------------------------
LaTex: No

< baseline >
setting LR(emb) LR(arch)        epo(emb,arch)                       En-micro  De-micro  En-macro  De-macro
Z2      -       e-3             -,10                                63.44     56.97     61.50     50.85
C2      -       e-3             -,2                                 62.21     59.26     58.21     48.32
C4      -       e-3             -,1                                 62.85     57.23     61.01     50.42

< setting 1 >
setting LR(emb) LR(arch)        epo(emb,arch)                       En-micro  De-micro  En-macro  De-macro
Z1      -       e-3             -,1                                 60.99     54.78     59.21     32.43
Enh.Z1  e-4     e-3             3,2                                 59.75     49.61     58.27     36.17
Enh.Z1  e-4     e-4             2,4                                 60.14     53.06     59.08     34.01
C1      -       e-3             -,1                                 60.26     55.48     56.98     31.96
Enh.C1  e-4     e-3             7,2                                 57.82     47.36     57.33     38.04
Enh.C1  e-4     e-2             OMIT FOR EFFICIENCY

< setting 3 >
setting LR(emb) LR(arch)        epo(emb,arch)                       En-micro  De-micro  En-macro  De-macro
Z3      -       e-3             -,1                                 62.03     55.38     60.71     49.62
Enh.Z3  e-4     e-3             2,2                                 61.53     48.62     60.47     47.08
Enh.Z3  e-4     e-4             2,4                                 62.12     48.01     61.04     46.13
C3      -       e-3             -,1                                 61.84     58.67     59.06     46.96
Enh.C3  e-4     e-4             OMIT FOR EFFICIENCY
Enh.C3  e-4     e-3             8,2                                 61.10     53.68     60.23     49.92
Enh.C3  e-4     e-2             OMIT FOR EFFICIENCY

< setting 4 >
setting LR(emb) LR(arch)        epo(emb,arch)                       En-micro  De-micro  En-macro  De-macro
Z4      -       e-3             -,1                                 62.86     56.43     60.74     50.37
Enh.Z4  e-4     e-3             2,2                                 60.56     48.43     59.99     47.12
Enh.Z4  e-4     e-4             2,3                                 61.48     51.74     60.34     49.87
C4      -       e-3             -,1                                 62.85     57.23     61.01     50.42
Enh.C4  e-4     e-4             OMIT FOR EFFICIENCY
Enh.C4  e-4     e-3             5,2                                 59.74     53.60     59.20     50.18
Enh.C4  e-4     e-2             OMIT FOR EFFICIENCY



-------------------------------------------------------------------------------------------------------------------------------------------
ARCH-ENHANCER DE 10% (5287)
-------------------------------------------------------------------------------------------------------------------------------------------
LaTex: Yes

< baseline >
setting LR(arch)  LR(emb)       epo(arch,emb)                       En-micro  De-micro  En-macro  De-macro
Z2      e-3       -             10,-                                63.44     56.97     61.50     50.85
C2      e-3       -             2,-                                 61.52     60.03     58.94     51.87

< setting 3 >
setting LR(arch)  LR(emb)       epo(arch,emb)                       En-micro  De-micro  En-macro  De-macro
Z3      e-3       -             1,-                                 62.03     55.38     60.71     49.62
Enh.Z3  e-3       e-4           7,1                                 64.48     56.10     63.11     50.42
*Enh.Z3 e-3       e-4           10,1                                64.01     56.28     62.72     50.53
* w/o rest of layers frozen
C3      e-3       -             1,-                                 59.88     57.71     58.30     48.23
Enh.C3  e-2       e-4           2,1                                 60.02     59.57     58.89     53.00
Enh.C3  e-3       e-4           2,1                                 62.29     60.07     61.09     52.83         > baseline
Enh.C3  e-4       e-4           6,1                                 63.36     59.34     61.64     51.59

< setting 4 >
setting LR(arch)  LR(emb)       epo(arch,emb)                       En-micro  De-micro  En-macro  De-macro
Z4      e-3       -             1,-                                 62.86     56.43     60.74     50.37
Enh.Z4  e-3       e-4           8,3                                 64.05     57.41     62.72     51.02
C4      e-3       -             1,-                                 61.82     57.45     60.88     50.11
Enh.C4  e-2       e-4           2,2                                 59.78     59.77     57.35     50.46
Enh.C4  e-3       e-4           1,1                                 62.88     59.97     61.32     51.96         ? > baseline
Enh.C4  e-4       e-4           8,2                                 63.18     59.08     61.58     51.97         ? > baseline

< setting CX >
** En-step as in C2 (trained with fixed BWE)
** De-step as in C3 (tune with unfrozen BWE)
setting LR(arch)  LR(emb)       epo(arch,emb)                       En-micro  De-micro  En-macro  De-macro
CX      e-3       e-4           1,-                                 63.05     59.08     60.41     50.99
Enh.CX  e-2       e-4           1,2                                 58.67     59.68     56.29     50.23
Enh.CX  e-3       e-4           1,1                                 63.09     59.46     61.42     52.25
Enh.CX  e-4       e-4           6,1                                 62.18     59.45     60.11     51.64

** observation: contrary to what EmbEnhancer seems to prove, tuned embs in this setting improves performance (comp. Z2 with Enh.Z3/Z4)
** observation: ArchEnhancer > fixed BWE > trainable BWE > EmbEnhacer
** next step: try on fine-tuning settings
** hypothesis: if tuned beforehand, sentiment-embs limit the capacity of arch. training and thus worsen the performance
** hypothesis: is it ok to tune embs to a fixed architecture?
** observation: Enh.C3 outperforms setting C2 w/ fixed BWE, C3 w/ trainable BWE
** observation: Enh.C4 outperforms original C4, C2 w/ fixed BWE, C3 w/ trainable BWE


-------------------------------------------------------------------------------------------------------------------------------------------
ARCH-ENHANCER DE 0.5% (264)
-------------------------------------------------------------------------------------------------------------------------------------------
LaTex: Yes

< baseline >
setting LR(arch)  LR(emb)       epo(arch,emb)                       En-micro  De-micro  En-macro  De-macro
Z2      e-3       -             10,-                                63.44     56.97     61.50     50.85
C2      e-3       -             2,-                                 62.21     59.26     58.21     48.32
C4      e-3       -             1,-                                 62.85     57.23     61.01     50.42

< setting 3 >
setting LR(arch)  LR(emb)       epo(arch,emb)                       En-micro  De-micro  En-macro  De-macro
Z3      e-3       -             1,-                                 62.03     55.38     60.71     49.62
Enh.Z3  e-3       e-4           7,1                                 64.48     56.10     63.11     50.42
*Enh.Z3 e-3       e-4           10,1                                64.01     56.28     62.72     50.53
* w/o rest of layers frozen
C3      e-3       -             1,-                                 61.84     58.67     59.06     46.96
Enh.C3  e-2       e-4
Enh.C3  e-3       e-4           2,1                                 63.42     59.64     60.68     49.22       about SOTA w/ De 20%
Enh.C3  e-4       e-4

< setting 4 >
setting LR(arch)  LR(emb)       epo(arch,emb)                       En-micro  De-micro  En-macro  De-macro
Z4      e-3       -             1,-                                 62.86     56.43     60.74     50.37
Enh.Z4  e-3       e-4           8,3                                 64.05     57.41     62.72     51.02
C4      e-3       -             1,-                                 62.85     57.23     61.01     50.42
Enh.C4  e-2       e-4
Enh.C4  e-3       e-4           2,3                                 62.60     59.76     59.86     50.82       > baseline
Enh.C4  e-4       e-4




-------------------------------------------------------------------------------------------------------------------------------------------
ATTENTION
-------------------------------------------------------------------------------------------------------------------------------------------
https://pypi.org/project/keras-self-attention/
LaTex: No

setting   attention   LR(arch)  LR(emb) epo-en  epo-de    En-micro  De-micro  En-macro  De-macro
Arch.C3   none        0.001     e-4     9,2     -,-       64.34     55.94     63.05     50.25
                                        -,-     2,1       62.29     60.07     61.09     52.83
Arch.C3   additive    0.001     e-4     8,2     -,-       63.18     57.89     60.79     50.72
                                        -,-     1,1       62.52     59.00     61.26     51.56
Arch.C3   loc.add     0.001     e-4     8,8     -,-       63.69     56.88     62.15     50.65
                                        -,-     1,1       61.77     59.65     60.03     51.75
Arch.C3   mult        0.001     e-4     6,2     -,-       63.95     57.68     62.39     50.27
          (none)                        -,-     1,1       61.45     59.29     59.88     51.19
Arch.C3   loc.mult    0.001     e-4     7,2     -,-       63.67     57.11     62.13     50.72
          (sigmoid)                     -,-     2,1       62.05     59.60     60.42     51.64
Arch.C3   loc.mult    0.001     e-4     7,2     -,-       63.84     56.58     62.35     50.68
          (tanh)                        -,-     2,1       62.36     59.62     60.46     51.39


-------------------------------------------------------------------------------------------------------------------------------------------
BETA EXPERIMENTS
-------------------------------------------------------------------------------------------------------------------------------------------
LaTex: No

<EN> CLARIN.si full (train 8696/16590/9415; dev 1863/3555/2019; test 1863/3555/2019)
<DE> CLARIN.si 10% (CLARIN_small10) (train 970/2952/1365; dev 2078/6326/2923; test 2078/6326/2923)

* [ [cnn(128,3)+maxpool][bilstm(128)+no dropout] ], dense(64), dense(64), C4 ArchEnhancer                            59.53  57.70  58.14  49.95
* [ [cnn(128,3)+maxpool][bilstm(128)+no dropout] ], dense(128), dense(64), C4 ArchEnhancer                           60.55  60.16  57.45  48.67
* [ [cnn(128,3)+maxpool][bilstm(128)+no dropout] ], dense(256), dense(128), C4 ArchEnhancer                          59.06  58.74  57.20  49.86
* [ [cnn(128,3)+maxpool][bilstm(128)+no dropout] ], loc.attention(10), dense(128), dense(64), C4 ArchEnhancer        61.50  59.35  60.27  51.34
* [ [cnn(128,3)+maxpool][bilstm(128)+no dropout] ], loc.attention(10), dense(128), dense(64), C3 ArchEnhancer        60.87  58.57  59.69  50.86
* [ [cnn(128,3)+maxpool][bilstm(128)+dropout(0.2)] ], loc.attention(10), dense(128), dense(64), C3 ArchEnhancer      60.97  59.40  58.96  50.77
* [ [cnn(256,3)+maxpool][bilstm(128)+no dropout] ], loc.attention(10), dense(128), dense(64), C4 ArchEnhancer        59.34  57.01  57.51  50.57
* [ [cnn(128,3)+maxpool][bilstm(128)+no dropout] ], glob.attention, dense(128), dense(64), C4 ArchEnhancer           60.58  57.32  59.60  50.83

                                                                En-mic  De-mic En-mac De-mac
* En train, EmbLayer fixed                            zero-shot: 63.35  58.25  61.33  51.06
  De train, EmbLayer tuned, rest fixed                fine-tune: 62.37  56.78  61.18  51.51
  zero-shot / De train, EmbLayer fixed
  (partial EmbEnhacer)



-------------------------------------------------------------------------------------------------------------------------------------------
TRIALS WITH DIFFERENT EMBEDDINGS
-------------------------------------------------------------------------------------------------------------------------------------------
LaTex: No

<EN> CLARIN.si full (train 8696/16590/9415; dev 1863/3555/2019; test 1863/3555/2019)
<DE> CLARIN full test (2078 neg/6326 neu/2923 pos)
<model architecture: bilstm(128) + dropout(0.2) + dense(64) + dense(64)> <batch=64>

                                        setting epoch En-micro  De-micro  En-macro  De-macro  comment
none, 100-dim                           Z1      1     60.99     54.78     59.21     32.43
(FastText) Collados(En), 100-dim        ZM      10    63.19     54.08     61.53     41.57
GloVe Twitter27B, 100-dim               ZM      5     63.02     57.90     61.01     38.53     De neu-skewed
GloVe Twitter27B, 200-dim               ZM      3     62.90     58.41     61.41     36.58     De neu-skewed
GloVe CommonCrawl42B, 300-dim           ZM      3     62.78     49.80     61.43     40.59
GloVe CommonCrawl840B, 300-dim          ZM      3     63.12     53.62     61.36     41.22


-------------------------------------------------------------------------------------------------------------------------------------------
SENTIMENT EMBEDDINGS TUNED WITH SENT140
-------------------------------------------------------------------------------------------------------------------------------------------
LaTex: No

                                        setting epoch En-micro  De-micro  En-macro  De-macro
baseline (Z2)                           Z2      10    63.44     56.97     61.50     50.85

filtered vocab absent from corpus       ZM      10    63.99     57.17     62.30     50.15
tune-bat.=256, eval. on 498 tweets
tune-LR=e-4

filtered vocab absent from corpus       ZM      10    64.04     57.72     62.47     50.03
tune-bat.=512, eval. on 498 tweets
tune-LR=e-4

filtered vocab absent from corpus       ZM      12    63.33     56.01     61.44     44.07
tune-bat.=512, eval. on 359 tweets
tune-LR=e-4, tune-task=binary
