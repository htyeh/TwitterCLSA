>Embeddings
En-De: weighted averaging from https://github.com/pedrada88/crossembeddings-twitter

>Links to datasets
SemEval: https://www.dropbox.com/s/byzr8yoda6bua1b/2017_English_final.zip?
CLARIN: https://www.clarin.si/repository/xmlui/handle/11356/1054
SB-10K: link: https://www.spinningbytes.com/resources/germansentiment/

>Available datasets (neg/neu/pos)
<EN> SemEval13-16 full (train 5383/15533/13724; dev 1153/3328/2941; test 1153/3328/2941)
<EN> SemEval13-16 oversampled (train 14079/15533/13724; dev 1153/3328/2941; test 1153/3328/2941) * merged with CLARIN neg train
<EN> SemEval13-16 downsampled (train 5383/6000/6000; dev 1153/3328/2941; test 1153/3328/2941)
<EN> CLARIN.si full (train 8696/16590/9415; dev 1863/3555/2019; test 1863/3555/2019)
<EN> CLARIN.si oversampled (train 16590/16590/16590; dev 1153/3328/2941; test 1153/3328/2941)
<EN> CLARIN.si downsampled (train 8696/9000/9415; dev 1863/3555/2019; test 1863/3555/2019)
<DE> CLARIN.si full (train 9697/29522/13643; dev 2078/6326/2923; test 2078/6326/2923)
<DE> CLARIN.si 20% (train 1939/5904/2729; dev 2078/6326/2923; test 2078/6326/2923)
<DE> CLARIN.si 10% (train 970/2952/1365; dev 2078/6326/2923; test 2078/6326/2923)
<DE> CLARIN.si 0.5% (train 48/148/68; dev 2078/6326/2923; test 2078/6326/2923)
<DE> SB-10k (989 neg/4131 neu/1509 pos)
<DE> SB-10k balanced (989 neg/1500 neu/1509 pos)
>Train/Dev/Test split: 70/15/15

>Tweet cleansing
1) removed repeating and 'Not Available' ones
2) contractions/elongated words are kept (own embeddings)
3) hashtags kept (might contain sentiment info)
4) to lowercase
5) removed @mentions/links
6) emoticon conversion (Keras Tokenizer does not encode emoticons; all mapped to English expressions acc. following link)
https://towardsdatascience.com/twitter-sentiment-analysis-using-fasttext-9ccd04465597
7) punctuation removal
8) converted \u2019 (to '), \u002c (to ,)

-------------------------------------------------------------------------------------------------------------------------------------------
COMPLETE EXPERIMENT DESCRIPTION
-------------------------------------------------------------------------------------------------------------------------------------------
1) baseline: train/dev/test on De
  a) scale De data & test for overfitting
2) lower & upper bound settings (suggested in https://arxiv.org/pdf/1905.07358.pdf)
  a) lo: initialize EmbLayer, test on De directly
  b) lo: fixed [En BWE], test on De directly (sanity check for overlapping words)
  c) hi: training on full De
  d) hi: fine-tuning on full De
3) fixed [EnDe BWE] (classical fine-tuning)
  a) fixed for training, unfreeze for De tuning
4) trainable [EnDe BWE] (classical fine-tuning)
  a) full De fine-tuning
  b) downsized De fine-tuning
5) fixed [EnDe BWE] (zero-shot)
6) *trainable [EnDe BWE] (zero-shot)
  a) reasons for underperformance
7) combine fixed + trainable [EnDe BWE]
  a) *zero-shot
  b) classical
8) BWE + architecture joint enhancement
  a) EmbEnhancer (train embs, then rest of architecture)
  b) ArchEnhancer (train architecture, then implement BWE tuning)
9) (optional) sentiment-tuning potential: are tuned embs really worse?
  a) building vocab only on train-data is not useful since most embs will be updated less than others
10) trainable [En BWE], map [De BWE] to [En BWE]
  a) MUSE/vecmap
  b) compare mapping evaluation
11) train En EmbLayer, map De to En space

* only En part of [EnDe BWE] get updates (no training on De vocab)

-------------------------------------------------------------------------------------------------------------------------------------------
COMPLETE EXPERIMENT SETTING INDEX
-------------------------------------------------------------------------------------------------------------------------------------------
(M-monolingual, C-classical, Z-zero-shot)
M1: EmbLayer
M2: [EnDe BWE] fixed
M3: [EnDe BWE] trainable
M4: [EnDe BWE fixed][EnDe BWE trainable]
Z1: En train/dev/test, EmbLayer; De test (zero-shot) -> De vocab should have random embeddings
ZM: En train/dev/test, [En BWE]; De test (zero-shot) -> De vocab should embeddings of 0's
Z2: En train/dev/test, [EnDe BWE] fixed; De test (zero-shot)
Z3: En train/dev/test, [EnDe BWE] trainable; De test (zero-shot)
Z4: En train/dev/test, [EnDe BWE fixed][EnDe BWE trainable]; De test (zero-shot)
C1: En train/dev/test, EmbLayer; De train/dev/test (fine-tuning)
C2: En train/dev/test, [EnDe BWE] fixed; De train/dev/test (fine-tuning)
C3: En train/dev/test, [EnDe BWE] trainable; De train/dev/test (fine-tuning)
CX: En train/dev/test, [EnDe BWE] fixed; De train/dev/test, [EnDe BWE] trainable (fine-tuning)
C4: En train/dev/test, [EnDe BWE fixed][EnDe BWE trainable]; De train/dev/test (fine-tuning)


-------------------------------------------------------------------------------------------------------------------------------------------
<model architecture: bilstm(128) + dropout(0.2) + dense(64) + dense(64)> <batch=64> <setting=Z2>
train             dev             test(En)      test(De)      F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)  pred. comment
SemEval(full)     SemEval(full)   SemEval(full) CLARIN(full)  64.16         51.26         60.48         50.23
SemEval(oversamp) SemEval(full)   SemEval(full) CLARIN(full)  62.76         29.67         57.18         29.01         De neg-skewed
SemEval(oversamp) SemEval(full)   SemEval(full) SB-10k(full)  64.17         25.33         60.45         25.22         De neg-skewed
SemEval(downsamp) SemEval(full)   SemEval(full) CLARIN(full)  58.36         46.43         57.21         46.77
CLARIN(full)      CLARIN(full)    CLARIN(full)  CLARIN(full)  63.44         56.97         61.50         50.85
CLARIN(oversamp)  CLARIN(full)    CLARIN(full)  CLARIN(full)  60.74         51.38         60.62         49.87
CLARIN(downsamp)  CLARIN(full)    CLARIN(full)  CLARIN(full)  59.83         51.38         59.39         50.20
-------------------------------------------------------------------------------------------------------------------------------------------
<model architecture: Medium post (https://medium.com/@panghalarsh/sentiment-analysis-in-python-using-keras-glove-twitter-word-embeddings-and-deep-rnn-on-a-combined-580646cb900a)> <batch=64> <setting=Z2>
train             dev             test(En)      test(De)      F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)  pred. comment   epoch
SemEval(full)     SemEval(full)   SemEval(full) CLARIN(full)  63.69         58.37         57.34         48.70                         12
SemEval(oversamp) SemEval(full)   SemEval(full) CLARIN(full)  63.58         24.99         57.62         21.56         De neg-skewed   9
SemEval(oversamp) SemEval(full)   SemEval(full) SB-10k(full)  59.46         28.89         53.40         26.99         De neg/En neu skewed  3
SemEval(downsamp) SemEval(full)   SemEval(full) CLARIN(full)  61.45         53.67         57.42         47.25                         9
CLARIN(full)      CLARIN(full)    CLARIN(full)  CLARIN(full)  62.75         58.39         60.66         48.30         De neu-skewed   9
CLARIN(oversamp)  CLARIN(full)    CLARIN(full)  CLARIN(full)  61.62         59.01         59.30         48.02         De neu-skewed   6
CLARIN(downsamp)  CLARIN(full)    CLARIN(full)  CLARIN(full)  61.08         57.77         59.81         50.14                         5
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> (uncleansed) SemEval full train (5383 neg/15533 neu/13724 pos)
<EN> (uncleansed) SemEval full dev (1153 neg/3328 neu/2941 pos)
<EN> (uncleansed) SemEval full test (1153 neg/3328 neu/2941 pos)
<DE> (uncleansed) CLARIN full test (2078 neg/6326 neu/2923 pos)
setting MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
Z1      30      128     0.2     2*64      -         64    1     62.19         55.15         54.32         28.28
ZM      30      128     0.2     2*64      -         64    7     63.35         52.53         59.75         40.85
Z2      30      128     0.2     2*64      -         64    8     63.60         51.67         59.69         49.73
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> SemEval full train (5383 neg/15533 neu/13724 pos)
<EN> SemEval full dev (1153 neg/3328 neu/2941 pos)
<EN> SemEval full test (1153 neg/3328 neu/2941 pos)
<DE> CLARIN full test (2078 neg/6326 neu/2923 pos)
Z1      30      128     0.2     2*64      -         64    1     62.14         52.14         57.69         34.79
ZM      30      128     0.2     2*64      -         64    8     63.66         51.02         59.17         40.55
Z2      30      128     0.2     2*64      -         64    7     64.16         51.26         60.48         50.23
Z3      30      128     0.2     2*64      -         64    1     63.84         51.98         59.25         47.65
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> SemEval oversampled train (oversampled w/ CLARIN neg train) (14079 neg/15533 neu/13724 pos)
<EN> SemEval full dev (1153 neg/3328 neu/2941 pos)
<EN> SemEval full test (1153 neg/3328 neu/2941 pos)
<DE> CLARIN full test (2078 neg/6326 neu/2923 pos)
setting MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)  pred. comment
Z1      30      128     0.2     2*64      -         64    1     60.30         23.96         57.20         18.84         De neg-skewed
ZM      30      128     0.2     2*64      -         64    6     62.49         24.95         59.60         22.10         De neg-skewed
Z2      30      128     0.2     2*64      -         64    5     62.76         29.67         57.18         29.01         De neg-skewed
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> SemEval oversampled train (oversampled w/ CLARIN neg train) (14079 neg/15533 neu/13724 pos)
<EN> SemEval full dev (1153 neg/3328 neu/2941 pos)
<EN> SemEval full test (1153 neg/3328 neu/2941 pos)
<DE> SB-10k full test (989 neg/4131 neu/1509 pos)
setting MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)  pred. comment
Z1      30      128     0.2     2*64      -         64    1     59.11         23.29         57.00         19.05         De neg-skewed
ZM      30      128     0.2     2*64      -         64    8     64.23         23.00         60.41         21.29         De neg-skewed
Z2      30      128     0.2     2*64      -         64    6     64.17         25.33         60.45         25.22         De neg-skewed
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> SemEval downsampled train (5383 neg/6000 neu/6000 pos)
<EN> SemEval full dev (1153 neg/3328 neu/2941 pos)
<EN> SemEval full test (1153 neg/3328 neu/2941 pos)
<DE> CLARIN full test (2078 neg/6326 neu/2923 pos)
setting MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
Z1      30      128     0.2     2*64      -         64    1     49.20         42.18         48.42         31.41
ZM      30      128     0.2     2*64      -         64    4     57.67         46.51         56.09         41.06
Z2      30      128     0.2     2*64      -         64    9     58.36         46.43         57.21         46.77
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> CLARIN downsampled train (8696 neg/9000 neu/9415 pos)
<EN> CLARIN full dev (1863 neg/3555 neu/2019 pos)
<EN> CLARIN full test (1863 neg/3555 neu/2019 pos)
<DE> CLARIN full test (2078 neg/6326 neu/2923 pos)
setting MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
Z1      30      128     0.2     2*64      -         64    1     55.91         49.46         56.07         35.17
ZM      30      128     0.2     2*64      -         64    8     59.91         48.88         59.69         43.26
Z2      30      128     0.2     2*64      -         64    4     59.83         51.38         59.39         50.20
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> CLARIN full train (8696 neg/16590 neu/9415 pos)
<EN> CLARIN full dev (1863 neg/3555 neu/2019 pos)
<EN> CLARIN full test (1863 neg/3555 neu/2019 pos)
<DE> CLARIN full test (2078 neg/6326 neu/2923 pos)
setting MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
Z1      30      128     0.2     2*64      -         64    1     60.99         54.78         59.21         32.43
ZM      30      128     0.2     2*64      -         64    10    63.19         54.08         61.53         41.57
Z2      30      128     0.2     2*64      -         64    10    63.44         56.97         61.50         50.85
Z3      30      128     0.2     2*64      -         64    1     62.03         55.38         60.71         49.62
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> CLARIN full train (8696 neg/16590 neu/9415 pos)
<EN> CLARIN full dev (1863 neg/3555 neu/2019 pos)
<EN> CLARIN full test (1863 neg/3555 neu/2019 pos)
<DE> SB-10k full test (989 neg/4131 neu/1509 pos)
setting MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)  pred. comment
Z1      30      128     0.2     2*64      -         64    1     60.65         65.44         59.39         44.94         De neu-skewed
ZM      30      128     0.2     2*64      -         64    10    63.36         66.55         61.82         53.23
Z2      30      128     0.2     2*64      -         64    10    63.75         66.58         62.25         59.29
Z3      30      128     0.2     2*64      -         64    1     62.97         68.38         61.27         58.76
-------------------------------------------------------------------------------------------------------------------------------------------
<EN> CLARIN oversampled train (16590 neg/16590 neu/16590 pos)
<EN> CLARIN full dev (1863 neg/3555 neu/2019 pos)
<EN> CLARIN full test (1863 neg/3555 neu/2019 pos)
<DE> CLARIN full test (2078 neg/6326 neu/2923 pos)
setting MAXLEN  bilstm  dropout dense     dropouts  batch epoch F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)  pred. comment
Z1      30      128     0.2     2*64      -         64    1     58.85         51.98         58.54         35.11         De neu-skewed
ZM      30      128     0.2     2*64      -         64    9     61.09         49.48         60.95         43.06
Z2      30      128     0.2     2*64      -         64    6     60.74         51.38         60.62         49.87

-------------------------------------------------------------------------------------------------------------------------------------------
HYPERPARAMETER SEARCH
-------------------------------------------------------------------------------------------------------------------------------------------
LaTex: No
<EN> CLARIN.si full (train 8696/16590/9415; dev 1863/3555/2019; test 1863/3555/2019)
<DE> CLARIN.si 10% (CLARIN_small10) (train 970/2952/1365; dev 2078/6326/2923; test 2078/6326/2923)
summary: BiLSTM 128/256, Dropout 0/0.2/0.4, Dense width/depth, batch 32/64/128

setting MAXLEN  bilstm  dropout dense     dropouts  batch epoch     F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
Z1      30      128     0.2     2*64      -         64    1         60.99         54.78         59.21         32.43
ZM      30      128     0.2     2*64      -         64    10        63.19         54.08         61.53         41.57

Z2      30      128     0.4     2*64      -         64    9         63.18         55.87         61.46         50.33
Z2      30      128     0.2     2*64      -         32    7         63.44         56.28         61.71         50.71
Z2      30      128     0.2     2*64      -         64    10        63.44         56.97         61.50         50.85
Z2      30      128     0.2     2*64      -         128   13        63.53         56.36         61.83         50.28
Z2      30      128(BiGRU) 0.2  2*64      -         64    7         63.39         57.90         61.37         51.93
Z2      30      128     -       2*64      -         64    8         63.23         56.66         61.36         49.81
Z2      30      128     0.2     128+64    -         64    11        63.31         56.25         61.64         50.37
Z2      30      128     -       128+64    -         64    8         63.21         56.73         61.16         49.38
Z2      30      128     0.2     4*64      -         64    10        63.40         56.38         61.65         50.42
Z2      30      256     0.2     2*64      -         64    8         63.12         55.69         61.50         49.48

Z3      30      128     0.2     2*64      -         64    1         62.03         55.38         60.71         49.62
Z3      30      128     0.2     4*64      -         64    1         62.43         57.40         60.40         49.00

setting MAXLEN  bilstm  dropout dense     dropouts  batch epoch(De) F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
C2      30      128     0.4     2*64      -         64    3         61.11         59.53         59.20         51.45
C2      30      128     0.2     2*64      -         32    3         61.62         59.59         60.07         52.06
C2      30      128     0.2     2*64      -         64    2         61.52         60.03         58.94         51.87
C2      30      128     0.2     2*64      -         128   3         61.19         59.30         59.24         50.71
C2      30      128(BiGRU) 0.2  2*64      -         64    3         61.04         59.46         59.40         52.53
C2      30      128     -       2*64      -         64    1         61.75         59.81         59.63         50.62
C2      30      128     0.2     128+64    -         64    2         61.50         58.91         59.83         50.50
C2      30      128     -       128+64    -         64    1         61.79         59.59         59.69         50.55
C2      30      128     0.2     4*64      -         64    2         61.84         59.32         59.80         51.44
C2      30      256     0.2     2*64      -         64    2         61.76         60.27         59.76         51.54

C3      30      128     0.2     2*64      -         64    1         59.88         57.71         58.30         48.23
C3      30      128     0.2     4*64      -         64    1         59.58         57.13         58.71         49.18   * De-micro worse than Z3

...
... TODO




>Issues/experiments to address
Tokenizer does not recognize De vocabs
  - fit_on_texts on both train(En) and test(De)
En: only vocab seen in training updated (generalization problem)
De embs not updated in zero-shot
1 reason for bad performance with updated embs: unable to generalize when only training embs updated
  - fit Tokenizer on train vocab only: train these into sentiment vectors




>>Older results
<EN> CLARIN downsampled train (8696 neg/9000 neu/9415 pos)
<EN> CLARIN full dev (1863 neg/3555 neu/2019 pos)
<EN> CLARIN full test (1863 neg/3555 neu/2019 pos)
<DE> SB-10k full test (989 neg/4131 neu/1509 pos)
setting MAXLEN  bilstm  dropout dense     dropouts  batch  epoch  F1-micro(En)  F1-micro(De)  F1-macro(En)  F1-macro(De)
Z1      30      32      -       2*64      -         32      1     57.46         62.80         57.94         46.96
Z1      30      64      0.5     2*64      -         32      1     55.43         60.90         55.63         48.05
Z1      30      64      -       2*64      -         32      1     54.50         60.28         54.71         48.84
Z1      30      64      -       2*128     -         32      1     54.44         58.25         54.49         46.69
Z1      30      128     -       2*128     -         32      1     56.63         62.83         56.79         46.99
ZM      30      64      -       2*64      -         32      8     58.69         57.78         58.84         50.84
ZM      30      64      -       2*128     -         32      6     57.86         57.38         58.00         51.08
ZM      30      128     -       2*128     -         32      5     58.09         60.02         58.21         52.83
Z2      10      -       -       1*128     -         16      2     57.02         59.10
Z2      10      -       -       3*128     -         16      1     57.04         60.36
Z2      30      32      -       2*32      0.4,-     32      6     58.41         59.12
Z2      30      32      -       2*64      0.4,-     32      9     58.53         58.72         57.86         54.01
Z2      30      64      -       2*64      0.4,-     32      8     60.00         60.72         59.56         55.78
Z2      30      64      -       2*128     0.4,-     32      7     60.05         60.82
Z2      30      128     0.8     2*64      0.4,-     32      8     59.71         59.08
Z2      30      128     0.5     2*64      0.4,-     32      6     59.54         59.57
Z2      30      128     0.1     2*64      0.4,-     32      7     60.64         62.50
Z2      30      128     0.1     2*64      0.2,-     32      6     61.18         62.90
Z2      30      128     0.1     128+64    0.4,0.2   32      10    58.83         59.38
Z2      30      128     0.1     128+64    0.4,-     32      7     61.18         64.26
Z2      30      128     0.1     128+64    0.2,-     32      5     59.69         60.28
Z2      30      128     0.1     2*128     0.4,-     32      10    60.35         63.10
Z2      30      128     0.1     2*128     0.2,-     32      5     60.18         60.89
Z2      30      128     0.1     2*128     -,-       32      2     60.21         59.54
Z2      30      128     -       2*128     0.4,-     32      5     60.37         61.76
Z2      30      128     0.1     2*256     0.4,-     32      2     58.09         59.38
Z2      30      256     -       256+128   0.4,-     32      5     59.98         61.63
Z2      30      256     -       2*256     0.4,-     32      7     60.75         62.62
Z2      30      256     0.1     2*256     0.2,-     32      7     59.86         60.82
Z2      10      512     0.5     2*128     -         16      7     55.63         55.90
Z2      10      128     0.5     4*32      -         32      9     56.77         58.55
Z2      10      256     0.5     4*64      -         32      6     56.75         58.80


>Experiments using CLARIN En full (12422 neg/23700 neu/13453 pos)
* best epoch num reported by Callback
setting MAXLEN  bilstm  lstm  dropout   dense   dropouts  batch   epoch F1-micro(En)  F1-micro(De)
M1      30      -       -     -         2*128   -         32      1     60.23         -
M1      30      256     -     0.9       2*128   -         32      1     60.62         -
M1      30      256     -     0.5       2*128   -         32      1     61.70         -
M1      30      256     -     0.2       2*128   -         32      1     61.48         -
M1      30      128     -     0.9       2*128   -         32      2     60.84         -
M1      30      128     -     0.5       2*128   -         32      1     60.67         -
M1      30      64      -     0.5       2*128   -         32      1     61.37         -
M1      30      64      -     0.5       2*256   -         32      1     59.90         -
M1      30      64      -     0.5       2*64    -         32      1     60.06         -
M1      30      32      -     0.5       2*32    -         32      1     61.22         -

M2      30      128     -     0.9       2*128   -         32      13    60.21         -
M2      30      128     -     0.5       2*128   -         32      14    60.39         -
M2      30      -       128   0.5       2*128   -         32      11    60.33         -
M2      30      -       64    0.5       2*64    -         32      13    59.15         -
M2      30      64      -     0.9       2*32    -         32      12    59.98         -
M2      30      64      -     0.9       2*32    -         128     15    58.32         -
M2      30      128     -     0.5       2*32    -         16      9     60.48         -
M2      30      128     -     0.5       2*64    -         16      11    59.94         -
M2      30      128     -     0.5       2*128   -         8       8     61.03         -
M2      30      128     -     0.2       2*128   -         8       4     60.56         -
M2      30      256     -     0.5       2*128   -         16      11    60.33         -
